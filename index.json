[{"categories":null,"contents":"Kasper Nissen ","permalink":"/about/kasper-nissen/","tags":null,"title":"Kasper Nissen"},{"categories":null,"contents":"Martin Jensen ","permalink":"/about/martin-jensen/","tags":null,"title":"Martin Jensen"},{"categories":["Raspberry Pi"],"contents":"It’s been a while, and I thought I would revisit my previous blog post on setting up a Raspberry Pi Kubernetes Cluster and publish an updated version of the guide with the latest version of Raspbian and Kubernetes. This is it. Some parts of this post are copy/paste from my previous post on setting up Kubernetes 1.9.0 Raspberry Pi cluster.\nMy Raspberry Pi cluster setup:  4pc Raspberry Pi (We used the Raspberry Pi 3 Model B) 4pc 16 GB MicroSDHC cards 1pc Small Switch (We used the d-link go-sw-5e) 4pc 0.3m Ethernet cables (we chose different colors for easy identification) 1pc USB Power Hub (We used Anker PowerPort 6 60W) 4pc Micro-USB cables 0.3m (approx 1ft)  The rack is custom made. We created an illustrator template and sent it to a laser cutting company. The template can be downloaded here.\nFlashing SD-cards Let’s start out flashing SD-cards. This process is boring and takes around 5 minutes per. SD-card. I prefer using etcher for flashing SD-cards, it’s super easy. You can find a copy here. We also need an operating system. Grab the latest version of Raspbian on either raspberrypi.org or directly from here (this is a link to the version I used).\nIf you haven’t already, go grab a cup of coffee, and repeat this process for all the nodes (SD-cards) you want in your cluster.\n**Flash it!\n**Insert the SD-card, use the image you just downloaded and then press: `flash`.\nOnce the image is flashed, take it out, and insert into the machine again (or just mount it again).\nEnable SSH\nNow we need to make SSH available on the machine. We do that by creating an empty file in the boot directory\n$ touch /Volumes/boot/ssh Unmount the SD-card and put it in the slot of a Raspberry Pi. Attach the network and power cables, and fire it up.\nAfter a minute or so, you should be able to SSH into the Raspberry Pi as follows:\n$ ssh pi@raspberrypi.local The default password for Raspbian is: raspberry\n**Setup hostname and attach static ip\n**Next, we want to make the Raspberry Pi easier to identify, and therefore provide it with a new hostname, and further assign it a static IP.\nIn order to make this as easy for you, I’ve create the following script, that you can copy to the Raspberry Pi and execute:\n$ nano hostname\\_and\\_ip.sh and insert the script:\nNow run the script, an example of my naming and ip convention can be seen below. But adapt to your liking.\nFirst argument: the new hostname\nSecond argument: the new static IP\nThird argument: the IP of your Router\nmaster: 192.168.1.100\n$ sh hostname\\_and\\_ip.sh k8s-master 192.168.1.100 192.168.1.1 worker-01: 192.168.1.101\n$ sh hostname\\_and\\_ip.sh k8s-worker-01 192.168.1.101 192.168.1.1 worker-02: 192.168.1.102\n$ sh hostname\\_and\\_ip.sh k8s-worker-02 192.168.1.102 192.168.1.1 worker-03: 192.168.1.103\n$ sh hostname\\_and\\_ip.sh k8s-worker-03 192.168.1.103 192.168.1.1 Now, reboot the Pi. You should be able to access the Pi over SSH as follows:\n$ ssh pi@k8s-master.local (or k8s-worker-01.local etc.) Verify that your Pi now also has a new static IP by running ifconfig.\nInstalling the prerequisites Now, that the static networking and naming is in place, we need to install some software on the Raspberry Pi.\nTherefore, create a new file on the Raspberry Pi:\n$ nano install.sh Copy and insert the following script.\nExecute the script\n$ sh install.sh This will install and configure docker, disable swap and install kubeadm.\nReboot the machine, and repeat this process for all your Raspberry Pis.\nInitialize the Kubernetes master So, we are now ready to set up Kubernetes. To do this, we are going to use the awesome tool called, kubeadm. This makes it pretty easy to spin up a Kubernetes cluster by, basically, running kubeadm init on the master node and kubeadm join on the worker nodes.\nOne of the purposes of this cluster is going to be demoing Kubernetes stuff. One example could be to pull out the network cable of one of the worker nodes and demoing how Kubernetes deals with this situation by rescheduling the pods from the lost node.\nTherefore, we would like to change one of the arguments to the kube-controller-manager, namely, pod-eviction-timeout which defaults to 5 minutes. That’s a long time to wait in a presentation. Instead, we want to change this to 10s. You may also want to change the time that Kubernetes allows the node to be unresponsive. It defaults to 40 seconds. To change this, add the following argument to the master configuration : node-monitor-grace-period: 10s.\nChanging arguments passed to the different Kubernetes core components by kubeadm is pretty simple. We just have to pass a YAML configuration file specifying the arguments we want to change. Let’s do that.\nCreate the configuration file:\n$ nano kubeadm\\_conf.yaml Copy and insert the following\napiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfiguration Save and run\n$ sudo kubeadm init --config kubeadm\\_conf.yaml This takes a couple of minutes. Once the process finishes you should see something similar to:\n... Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026quot;kubectl apply -f \\[podnetwork\\].yaml\u0026quot; with one of the options listed at: [https://kubernetes.io/docs/concepts/cluster-administration/addons/](https://kubernetes.io/docs/concepts/cluster-administration/addons/) You can now join any number of machines by running the following on each node as root: kubeadm join --token TOKEN 192.168.1.100:6443 --discovery-token-ca-cert-hash HASH Follow the instructions in the output:\n$ mkdir -p $HOME/.kube $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ sudo chown $(id -u):$(id -g) $HOME/.kube/config You can verify that your master node is up and running.\npi@master:~ $ kubectl get nodes NAME STATUS ROLES AGE VERSION master NotReady master 3m v1.9.0 Don’t mind the status being NotReady. In order for the master node to become ready, we need to install a container network. But before we do that, let’s add some more nodes to the cluster first.\nSetting up the worker nodes Alright, next up we need to spin up some workers for the cluster to be complete.\nAssuming you have already set up the prerequisites mentioned above, we basically only need to run the kubeadm join on each of your worker nodes. As shown above, kubeadm outputs the command that you need to run on all your worker nodes.\n$ sudo kubeadm join --token TOKEN 192.168.1.100:6443 --discovery-token-ca-cert-hash HASH Repeat for every node.\nSet up weave as the container network Nodes are not able to communicate without a container network, which is something you have to provide. Therefore, the final piece of the puzzle is to add one. We will be using weave-net for this. On the master node, run the following command:\n$ kubectl apply -f “[https://cloud.weave.works/k8s/net?k8s-version=$(kubectl](https://cloud.weave.works/k8s/net?k8s-version=$%28kubectl) version | base64 | tr -d ‘\\\\n’) All set and done… That was it. You should now have a fully functioning Raspberry Pi Kubernetes cluster. Verify your setup by running\npi@k8s-master:~ $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready master 17m v1.11.2 k8s-worker-01 Ready \u0026lt;none\u0026gt; 10m v1.11.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 10m v1.11.2 k8s-worker-03 Ready \u0026lt;none\u0026gt; 6m v1.11.2 ","permalink":"/posts/2018-09-14_setting-up-a-kubernetes-1-11-raspberry-pi-cluster-using-kubeadm-952bbda329c8/","tags":null,"title":"Setting up a Kubernetes 1.11 Raspberry Pi Cluster using kubeadm"},{"categories":["Raspberry Pi"],"contents":"Apparently it’s that time of the year again. Approximately a year ago I published a “state-of-the-art” how to run a Raspberry Pi Kubernetes cluster using HypriotOS and Kubernetes 1.4. Lots of things has happen during the past year, and I thought it was time to play with the Raspberry Pi cluster again. This blog post will guide you through the process of setting up a Raspberry Pi Kubernetes cluster on the latest version of Raspbian, and with the latest version of Kubernetes, which is 1.9.0.\nThis blog post was inspired by the great work of Docker Captain Alex Ellis.\nLet’s get started!\nInitial setup of SD-cards and prerequisites The following section is pretty generic. This flow has to be repeated for every node you want to join your cluster.\nFlashing the SD card The first thing we need is an image. Grab the latest Raspbian OS here (.zip). I use etcher by Resin.io for flashing the SD-cards. It doesn’t get easier than this.\nJust select the .zip file you just downloaded, and insert your SD-card, select the drive, and click “Flash!”. Easy, right?\nOnce the flashing process is finished, we need to add an empty file to the drive. Therefore, take the SD-card out and insert it again. We are going to create an empty file called ssh. This allow us to SSH into the node once booted.\n$ touch /Volumes/boot/ssh Now, take out the SD-card and insert it into the Raspberry Pi. Attach network cable and power, and boot up the little machine.\nAfter a minute or so, you should be able to access the Raspberry Pi from the same network.\n$ ssh pi@raspberrypi.local Default raspbian password is: raspberry\nChange the hostname and assign a static IP Next, we will configure the Raspberry Pi, so that it’s easy to identify and access. We will therefore change the hostname, and assign a static IP to the node. In order to make this process a bit easier, I have made a simple script for you.\nIn order to run it, create a new file on the Pi:\n$ nano hostname\\_and\\_ip.sh Copy the following into the new file\nAnd run the script (below is the naming and IP convention used in my setup, you should adapt this to your network setup)\nFirst argument: the new hostname\nSecond argument: the new static IP\nThird argument: the IP of your Router\nmaster: 192.168.1.100\n$ sh hostname\\_and\\_ip.sh master 192.168.1.100 192.168.1.1 worker-01: 192.168.1.101\n$ sh hostname\\_and\\_ip.sh worker-01 192.168.1.101 192.168.1.1 worker-02: 192.168.1.102\n$ sh hostname\\_and\\_ip.sh worker-02 192.168.1.102 192.168.1.1 worker-03: 192.168.1.103\n$ sh hostname\\_and\\_ip.sh worker-03 192.168.1.103 192.168.1.1 Now, reboot the Pi. You should be able to access the Pi over SSH as follows:\n$ ssh pi@master.local (or worker-01.local etc.) Verify that your Pi now also has a new static ip by running ifconfig.\nInstalling the prerequisites Now, that node networking and naming is in place, we need to install some software on the Raspberry Pi. First of all we need docker, secondly we need to add the kubernetes repo to the repo list, disable swap memory, and lastly install kubeadm. Again, in order to make this process easy for you as a reader, all of this can be run as a script.\nTherefore, create a new file on the Raspberry Pi:\n$ nano init.sh Copy and insert the following script.\nI experienced some issues with a change in the kernel related to cgroups_memory. The current solution is reflected in the script above. However, if you encounter issues, you might want to change cgroup_memory=1 to cgroup_enable=memory. For now, however, cgroup_memory=1 is working. More on this can be found here.\nRun the script,\n$ sh init.sh Reboot the machine and SSH back into the Raspberry Pi once the reboot has finished.\n$ sudo reboot That was all the prerequisites and generic setup of all the Raspberry Pis. Remember to repeat this process for every node in the cluster.\nNow, let’s move on to setting up our Kubernetes master.\nInitialize the Kubernetes master So, we are now ready to set up Kubernetes. To do this, we are going to use the awesome tool called, kubeadm. This makes it pretty easier to spin up a Kubernetes cluster by, basically, running kubeadm init on the master node and kubeadm join on the worker nodes.\nOne of the purposes of this cluster is going to be demoing Kubernetes stuff. One example could be to pull out the network cable of one of the worker nodes and demoing how Kubernetes deals with this situation by rescheduling the pods from the lost node.\nTherefore, we would like to change one of the arguments to the kube-controller-manager, namely, pod-eviction-timeout which defaults to 5 minutes. That’s a long time to wait in a presentation. Instead, we want to change this to 10s.\nChanging arguments passed to the different Kubernetes core components by kubeadm is pretty simple. We just have to pass a YAML configuration file specifying the arguments we want to change. Let’s do that.\nCreate the configuration file:\n$ nano kubeadm\\_conf.yaml Copy and insert the following\napiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfiguration You may wish to change the time that Kubernetes allows the node to be unresponsive as well. It defaults to 40 seconds. To change this, add the following argument to the master configuration above: node-monitor-grace-period: 10s.\nSave and run\n$ sudo kubeadm init --config kubeadm\\_conf.yaml This takes a couple of minutes. Once the process finishes you should see something similar to:\n... Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster.\nRun kubectl apply -f \\[podnetwork\\].yaml with one of the options listed at:\nhttps://kubernetes.io/docs/concepts/cluster-administration/addons/\nYou can now join any number of machines by running the following on each node\nas root:\nkubeadm join --token TOKEN 192.168.1.100:6443 --discovery-token-ca-cert-hash HASH Follow the instructions in the output:\n$ mkdir -p $HOME/.kube\n$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n$ sudo chown $(id -u):$(id -g) $HOME/.kube/config\nYou can verify that your master node is up and running.\npi@master:~ $ kubectl get nodes\nNAME STATUS ROLES AGE VERSION\nmaster NotReady master 3m v1.9.0\nDon’t mind the status being NotReady. In order for the master node to become ready we need to install a container network. But before we do that, let’s add some more nodes to the cluster first.\nSetting up the worker nodes Alright, next up we need to spin up some workers for the cluster to be complete.\nAssuming you have already set up the prerequisuites mentioned above, we basically only need to run the kubeadm join on each of your worker nodes. As shown above, kubeadm outputs the command that you need to run on all your worker nodes.\n$ sudo kubeadm join \u0026ndash;token TOKEN 192.168.1.100:6443 \u0026ndash;discovery-token-ca-cert-hash HASH\nRepeat for every node.\nSet up the container network Nodes are not able to communicate without a container network, which is something you have to provide. Therefore, the final piece of the puzzle is to add one. We will be using weave-net for this. On the master node, run the following command:\n$ kubectl apply -f https://git.io/weave-kube-1.6\nAll set and done… That was it. You should now have a fully functioning Raspberry Pi Kubernetes cluster. Verify your setup by running\npi@master:~ $ kubectl get nodes\nNAME STATUS ROLES AGE VERSION\nmaster Ready master 55m v1.9.0\nworker-01 Ready 34m v1.9.0\nworker-02 Ready 14m v1.9.0\nworker-03 Ready 5m v1.9.0\nLast thing we need to verify is that the configuration of the controller-manager we specified earlier. Did the pod-eviction-timeout get parsed to the actual kube-controller-manager process? This is easy to test, just run\n$ kubectl describe pod kube-controller-manager-master -n kube-system\nYou should see something similar to this\n\u0026hellip;\n\u0026hellip;\nContainers:\nkube-controller-manager:\nContainer ID: docker://f9f4077d9008690f04169932c02fba79c8d446b71a347f75c2f6aa96f36587ca\nImage: gcr.io/google_containers/kube-controller-manager-arm:v1.9.0\nImage ID: docker-pullable://gcr.io/google_containers/kube-controller-manager-arm@sha256:167a5468244114c484856bfe995cc3630678f0ab2aa85cf9d1ab6341fd04b8e5\nPort: Command:\nkube-controller-manager\n--pod-eviction-timeout=10s\n\u0026ndash;kubeconfig=/etc/kubernetes/controller-manager.conf\n\u0026ndash;service-account-private-key-file=/etc/kubernetes/pki/sa.key\n\u0026ndash;cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt\n\u0026ndash;cluster-signing-key-file=/etc/kubernetes/pki/ca.key\n\u0026ndash;address=127.0.0.1\n\u0026ndash;leader-elect=true\n\u0026ndash;use-service-account-credentials=true\n\u0026ndash;controllers=*,bootstrapsigner,tokencleaner\n\u0026ndash;root-ca-file=/etc/kubernetes/pki/ca.crt\nState: Running\nStarted: Wed, 20 Dec 2017 18:52:13 +0000\nReady: True\nRestart Count: 0\n\u0026hellip;\n\u0026hellip;\nAs you can see the parameter was passed correctly to the kube-controller-manager.\nThat was all for this time folks. Happy hacking.\n","permalink":"/posts/2017-12-20_setup-a-kubernetes-1-9-0-raspberry-pi-cluster-on-raspbian-using-kubeadm-f8b3b85bc2d1/","tags":null,"title":"Setup a Kubernetes 1.9.0 Raspberry Pi cluster on Raspbian using Kubeadm"},{"categories":["Conference"],"contents":"The second day of breakout sessions. Crazy walk-up line before the first session of the day with Sam Newman on Building Microservices on AWS. Luckily I got up early and queued up 70 minutes before the talk. At this point, 20–30 people were already lined up. But I managed to get in.\nBuilding Microservices on AWS Sam started out by defining microservices as:\n “Independently deployable services that work together, modelled around a business domain”\n Alright, moving on. The Gartner Hype Cycle was up next.\nIt puts Microservices at the peak of inflated expectations and SOA (Service Oriented Architecture) at the Through of Disillusionment. These two buzzwords is sort of the same thing.\nBack to microservices and the definition mentioned above. Sam pointed out that the most important part of the definition is independently deployability which leads to autonomy, and allows for replaceability of components.\nThen Sam went on talking about how Amazon back in the day realized they needed to split teams up into smaller teams in order to reduce coordination and allowing people to move fast.\n “No, communication is terrible!” — Jeff Bezos\n Autonomy is all about reducing coordination and allowing people to be fast in doing what they need to do. Sam spoke about communication pathways, the more people, the more communication pathways, and the higher the need to coordinate.\nSo the takeaway here is, coordination will slow you down, and building small autonomous teams will speed up your process because of the limited communication pathways. Further teams work within the service boundary and mostly only needs to communicate using an API.\nAnother highlight, was the single most common problem when building microservices; services reaching straight into the database of another service instead of communicating through the API. Don’t do this. Don’t allow other parties to use your backdoor, you won’t be able to detect all the things they are doing. Only let them use the front door.\n Microservices are an architecture which optimise for autonomy\n Once you have decided to built microservices, next up is how you then run and manage them in production. Sam talked about the higher level abstractions of the cloud such as Elastic Beantalk, if you are a small shop, or maybe AWS ECS if you package your applications into docker containers.\nSam also mentioned the history of Borg at Google, and how all these lessons learned ended up in the open source container scheduler, Kubernetes.\nHe also highlighted some the possible problems of using container orchestration. If you don’t take full advantage of the platform, you may end up with one container starving the resources on a node, resulting in the rest of the applications failing because of starvation. Containers can get in each other’s way when running multi-tenant systems.\nLast, but not least, was a small definition of serverless, and that many AWS services, such as Dynamo, S3, and Lambda can be seen as serverless tools. Again, serverless is a higher level abstraction where developers don’t have to think about the underlying infrastructure. But, be careful, if running a hybrid solution, such as functions using the same database, you may end up killing your database because of the lack of throttling. Instead, use serverless database tools, such as Dynamo.\nAs always, Sam Newman is a really good and insightful presenter — and this performance was no exception.\nBuilding Effective Container Images The second talk of the day featured Abby Fuller, AWS, and Prakash Janakiraman, Nextdoor.\nAbby started out by providing an introduction of how layers in Docker works, with the read-only base layers, and the thin read-write layer on top. Why do I care how many layers I have? Well, more layers result in larger images. And you don’t want large docker images. Smaller images mean faster builds and deploys. Further, it will limit the surface of attack.\n**_How can I reduce my layers? _**Sharing is caring. Use shared base layers where possible in order to reuse as many of the layers as possible. Don’t use a RUN statement for each cmd you want to run. Combine them in one statement instead, so keep them it in one layer.\nAnother good practice is to use minimal base images. Instead of using Ubuntu for a python app, use the python alpine. You probably don’t need the entire OS.\nFurther, use the Docker cache. Cache rules everything, which means that the order of statement really matters. Place the dynamic statements as far down in the Dockerfile as possible.\nAfter Abby’s introduction and best practices for optimizing container images, Prakash from Nextdoor talked about how they were using these techniques at Nextdoor. Nextdoor is a social network for neighborhood. They started out with a python/django monolithic applications, but have been transitioning into a microservice oriented architecture with Go as the programming language. They moved from pre-baking AMI’s to utilizing Docker Containers.\nGreat presentation, even though it was fairly short, but then there was time for questions from the audience.\nContainerised Machine Learning on AWS The thirds and last talk of the day featured Asif Khan, Hokuto Hoshi, Yuichiro Someya.\nAsif started out by giving an introduction to machine learning and all the different tools that AWS provides.\nFurther, Asif showed an example of machine learning workflow. This slide below shows all the different stakeholders and how the services work together in order to run the machine learning models.\nThe last part of the presentation featured experiences on running a machine learning platform on AWS from Cookpad.\nCookpad needed to classify images from the Photo Roll in order to only upload pictures of food and make them available within their service. It was a great presentation, even though I don’t know much about machine learning and image classification, but it’s definitely something I will be looking more into in the future.\nAll in all, three very intersting talks. Further the expo opened today. Spent a couple of hours there talking to many different companies. It’s massive!\n","permalink":"/posts/2017-11-29_aws-re-invent-2017-day-2-5d4c113a70de/","tags":null,"title":"AWS re:invent 2017 — day 2"},{"categories":["Conference"],"contents":"The focus of my first day at Reinvent was mostly on the topic of security and financial services. Re:invent is very crowded and there’s crazy lines for some of the sessions. Note to self: If you don’t have a reserved seat — come early (like really early). In the following I will give a short write up of the breakout sessions that I attended.\nAll right, let’s get started.\nSecurity Anti-Patterns: Mistakes to Avoid The first talk of the day featured Kurt Gray, Global Solutions Architect for Financial Services at AWS, and Jonathan Baulch, Director, Architecture at Fidelity Investments.\nKurt started out by defining a anti-patterns, with the following great slide.\nNext, he presented a bunch of security anti-patterns. In the following, I will try to highlight some of these and present the takeaways from this presentation.\n**_Personally Owned AWS Accounts\n_**Don’t sign your accounts up on individual people, what happens if they don’t show up some day? Instead use group distribution lists for emails, etc. Further, use MFA devices that everybody can get their hands on.\n**_AWS Account Overcrowding\n_**Don’t overcrowd your AWS account. It makes auditing a lot harder, further, the blast radius will be a lot bigger if compromised.\nInstead, use a multi-account strategy. It will limit exposure if something gets compromised. Think of your AWS accounts as single family homes\n**_Trusted IP Access w/o Client Auth\n_**Routing is not security. Implement and use a proper authentication methods instead.\n**_Network Egress Backhauling\n_**Limit access to resources instances can access by filtering traffic. An example of this is to use an exit VPC to restrict egress.\n**_Security Questionnaires\n_**Instead of using home-brewed questionaries, Use standardized controls, e.g., ISO certifications, etc.\n**_Manual Technical Auditing\n_**How are you auditing yourself? Stop using manual steps, instead automate this process using Continuous Automated Auditing.\nOver-the-wall Software Delivery method Over-the-wall software delivery method is where dev, QA, and ops are kept separate and software is handed off between departments. Instead, you should move towards a DevOps model with small interdisciplinary delivery teams and where developers are on-call. You built it, you run it.\nAdd Security and enter DevSecOps. Proactive security checking, penetration tests etc.\nAfter the presentation by Kurt, Jonathan from Fidelity Investments took over and talked about how they do DevSecOps at Fidelity Investments.\nHe talked about the three different layers of security; Prevention, Detection, and Remediation. Further, he introduced the command line tool cfn_nag, a linting tool for CloudFormation Templates that will help you catch holes in the pipeline before reaching production.\n_Key-takeaways\n_Standard Controls: Prescriptive. Certifiable.\nManaged Services: Consistent Controls, Less overhead\nDevSecOps practices: Faster Delivery, Faster Patching, Faster Innovation.\nCulture Shift: How to Move to Global Financial Services Organization to a DevOps Operating Model The second talk of the day featured Alan Garver, Mahdi Sajjadpour, and Jonathan Sywulak and focused on how to move a global financial service organization to a DevOps operating model. Alan started out by listing different categories of challenges\n Tooling challenges Organization challenges Financial enterprises challenges  Back in the 2000s, Amazon was built as a monolithic application. They made the transition to a service-oriented architecture and shifted two the 2 pizza teams organization. These teams should only be responsible for their product.\nSome key elements in this transformation includes\n Move from manual handoffs to “as a service” Automate all the things Simplify and decompose monoliths  Further, this transformation also included incorporating infrastructure code relevant to a specific service to live alongside the source code.\nEnabling self-service at scale\nHow do you then enable all the necessary tools for enabling developers to be self-driving and autonomous while still being compliant and secure?\nWhat does self-service infrastructure enable?\n Faster innovation Repeatable Scalable SecureLeast privilege Testable Immutable  How to get to self-service?\nWe need to make sure that developers can get the resources they need.\nOne way to enable self-service while keeping everything tight is to use Cloud Formation Templates for making things accessible in the AWS Service Catalog (standardized patterns, pre-approved)\nThe presentation further included a demo of how to build a unified interface for providing developers with the tools they need while still being conformant with company policies. Again the cfn_nag tool was highlighted as a great way for ensuring policies for Cloud Formation templates.\nTo summarize this talk; you can provide your developers (your customers) with the tools they need in order to fast and easily deploy their features, while still enforcing policies.\nGetting Started with Amazon Aurora The third talk of the day was an introduction to Amazon Aurora. I’ve been following the recent news of the Postgres compatibility and was very excited to learn more. This presentation featured Gurmit Ghatore, Brandon O’Brien, and Debanjan Saha.\nWhat is Aurora?\nAurora is a fairly new database offering from AWS and can be a drop-in replacement for MySQL and Postgres. It promises a lot! Better performance, higher availability, and cheaper. And all this is delivered as a managed services on AWS.\nThe more technical side of things includes a scale-out and distributed design, built as a service oriented architecture, which will automate a lot of administrative tasks. Aurora automatically replicas over 3 Availability zones and always keeps 6 copies in order to tolerate zone failures.\nIt integrates with many of AWS other cloud services such as S3, lambda, IAM, etc. And as it is a drop-in replacement for MySQL and Postgres — all third-party tools should be compatible as well.\nIn terms of performance compared to Postgres and MySQL, they promise a 3x improvement over Postgres and 5x compared to MySQL.\nAWS Aurora customers have migrated to Aurora from Cassandra and have seen significant savings in cost and management.\nHow did they achieve this?\nBasically, they are doing less and have made some significant improvements in terms of processing work asynchronously.\nThe last part of this presentation included a story of how Expedia had migrated from a setup that included Cassandra to using Aurora instead. They saw cost savings vs. Cassandra at 10–15% and minimized the management task of keeping everything up significant. Aurora further lets them scale very easy by adding more read replicas.\nBorn in the AWS Cloud: How Eagle Genomics Uses AWS to Process Billions of DNA Sequence Reads The last talk of the day featured Raminderpal Singh and Nick James. This presentation focused on how they had built a pipeline for processing billions of workflows.\nThey leveraged a tool called eHive and explained further how they had moved to Docker and Docker Swarm to orchestrate the processing of the workflows.\nI must admit, I was a bit tired at the end and the write up of this presentation is fairly short.\nGreat first day at Re:invent even though it didn’t include much talk about containers nor Kubernetes. I learned a ton — and will definitely go back home and investigate and try out AWS Aurora. It really sounds like a no-brainer if you are already using AWS RDS Postgres or MySQL.\n","permalink":"/posts/2017-11-28_aws-re-invent-2017-day-1-c4cdb5955dfc/","tags":null,"title":"AWS re:invent 2017 — Day 1"},{"categories":["Conference"],"contents":"Welcome to Las Vegas\nOur first full day in Las Vegas is over. It’s been a great day with time to explore the city and be a tourist before kicking off the AWS re:invent conference tomorrow.\nRe:invent is going to be big. 40k-45k attendees. I’m guessing over 3/4 of the passengers of yesterdays flight from Stockholm to Las Vegas were attendees flying in. I am really looking forward to this awesome conference.\nAs mentioned before, today was more about being a tourist, rather than an AWS re:invent attendee. We had pre-booked a helicopter tour over Grand Canyon — and what a great experience. It was my first time flying in a helicopter. We chose the “Golden Eagle Air Tour” from Papillon. The tour includes views of the Las Vegas Strip, Hoover Dam, Lake Mead, The Colorado River, and of course the Grand Canyon. And not to forget pick-up and drop-off by a stretch limo (we are in freaking Vegas).\nHoover DamGrand CanyonGrand CanyonLas Vegas — StratosphereLas Vegas — Trump tower (gold of course 🤦‍)Las Vegas BoulevardThe MGM Grand (the hotel we stays at)\nAlright, enough with the tourist pictures. That’s not why we travelled to Las Vegas. We are here for AWS re:invent. After the helicopter ride, we picked up our conference badges at the MGM Grand, and we are now ready to start learning tomorrow.\n… sorry for the “not so” technical, nor detailed post. Stay tuned for kickoff — and more details about AWS re:invent once it kicks off.\n","permalink":"/posts/2017-11-27_aws-re-invent-2017-day-0-e91baef32d54/","tags":null,"title":"AWS re:invent 2017 — Day 0"},{"categories":["Kops"],"contents":"The Kubernetes release cadence is fast-paced with minor releases every quarter. Awesome! But how do I keep up?\nDon’t worry, Kops makes it fairly easy to update your HA production cluster without any downtime (assuming you have scaled your deployments to a minimum of 2 pods per deployment).\nThis blog post will walk you through upgrading your Kubernetes cluster version 1.6.2 cluster to Kubernetes cluster 1.7.10. The following will (most-likely) be applicable with other versions as well. Before any production upgrade, some testing and walkthrough of the gameplan is always a good idea. We currently have three Kubernets clusters managed by Kops on AWS; dev, staging, and production. However, all environments are actively being used on a daily basis. Shutting down a cluster a day or two for testing is really not an option. Instead, Kops makes it incredibly easy to test your upgrade process by making it easy to spin up new clusters with specific configurations.\nIn the following section I will provide you with a hands-on description of how you can upgrade you cluster without any downtime.\nEnough with the introduction, let’s get down to business!\nGo to the Kops git-repo and download Kops version 1.6.2, or just click on this link: https://github.com/kubernetes/kops/releases/tag/1.6.2\nIf you haven’t set up a Kubernetes Kops cluster before, I will advise you to go have a look at some of my previous posts about this subject. (Links: HA Kubernetes Cluster on AWS? — Kops makes it easy and Setting up a Highly Available Kubernetes Cluster with private networking on AWS using Kops)\nIn the following I assume that you have setup all the prerequisites for spinning up a Kops cluster, such as an S3 bucket, Route53, etc.\nLet’s get started.\nSet up your environment variables for the Kops, and configure you AWS profile (I’m using multiple AWS configs) as follows:\nexport KOPS_STATE_STORE=”s3://path_to_your_bucket”\nexport KOPS_NAME=name_of_your_new_cluster\nexport PRIVATE_HOSTED_ZONE_ID=id_of_your_route53_private_zone\nexport AWS_PROFILE=your_aws_profile\nSpin up a new cluster with Kops 1.6.2 installed using the following configuration\nkops create cluster \\\n\u0026ndash;name $KOPS_NAME \\\n\u0026ndash;state $KOPS_STATE_STORE \\\n\u0026ndash;node-count 3 \\\n\u0026ndash;zones eu-west-1a,eu-west-1b,eu-west-1c \\\n\u0026ndash;master-zones eu-west-1a,eu-west-1b,eu-west-1c \\\n\u0026ndash;dns-zone=${PRIVATE_HOSTED_ZONE_ID} \\\n\u0026ndash;dns private \\\n\u0026ndash;node-size t2.medium \\\n\u0026ndash;master-size t2.small \\\n\u0026ndash;topology private \\\n\u0026ndash;networking weave \\\n\u0026ndash;image kope.io/k8s-1.6-debian-jessie-amd64-hvm-ebs-2017-05-02 \\\n\u0026ndash;kubernetes-version=1.6.2 \\\n\u0026ndash;yes\nThis will spin up a HA kubernetes cluster with three master nodes spread across three availability zones, and three worker nodes, also spread across three availability zones. Wait for the cluster to spin up and verify that you can get a list of nodes (the above will spin up a private cluster and you therefore need to add a public DNS entry in Route53 to access the Kubernetes api from the internet. Just duplicate the entry Kops created in the private zone in the public zone):\n$ kubectl get nodes -L kubernetes.io/role\nNAME STATUS AGE VERSION ROLE\nip-172-20-126-152.eu-west-1.compute.internal Ready 1m v1.6.2 master\nip-172-20-127-107.eu-west-1.compute.internal Ready 36s v1.6.2 node\nip-172-20-55-11.eu-west-1.compute.internal Ready 1m v1.6.2 master\nip-172-20-63-108.eu-west-1.compute.internal Ready 20s v1.6.2 node\nip-172-20-69-180.eu-west-1.compute.internal Ready 22s v1.6.2 node\nip-172-20-89-92.eu-west-1.compute.internal Ready 2m v1.6.2 master\nVerify the kubernetes version with\n$ kubectl version\nClient Version: version.Info{Major:\u0026ldquo;1\u0026rdquo;, Minor:\u0026ldquo;7\u0026rdquo;, GitVersion:\u0026ldquo;v1.7.4\u0026rdquo;, GitCommit:\u0026ldquo;793658f2d7ca7f064d2bdf606519f9fe1229c381\u0026rdquo;, GitTreeState:\u0026ldquo;clean\u0026rdquo;, BuildDate:\u0026ldquo;2017-08-17T08:48:23Z\u0026rdquo;, GoVersion:\u0026ldquo;go1.8.3\u0026rdquo;, Compiler:\u0026ldquo;gc\u0026rdquo;, Platform:\u0026ldquo;darwin/amd64\u0026rdquo;}\nServer Version: version.Info{Major:\u0026ldquo;1\u0026rdquo;, Minor:\u0026ldquo;6\u0026rdquo;, GitVersion:\u0026ldquo;v1.6.2\u0026rdquo;, GitCommit:\u0026ldquo;477efc3cbe6a7effca06bd1452fa356e2201e1ee\u0026rdquo;, GitTreeState:\u0026ldquo;clean\u0026rdquo;, BuildDate:\u0026ldquo;2017-04-19T20:22:08Z\u0026rdquo;, GoVersion:\u0026ldquo;go1.7.5\u0026rdquo;, Compiler:\u0026ldquo;gc\u0026rdquo;, Platform:\u0026ldquo;linux/amd64\u0026rdquo;}\nUpgrading your cluster configuration We are now ready to update our cluster. Kops comes with a simple feature for upgrading your cluster with the upgrade command.\nBefore we run the update command we need to go fetch the latest version of Kops (in the time of writing 1.7.1, link: https://github.com/kubernetes/kops/releases/tag/1.7.1)\nNow run the upgrade command as follows:\n$ kops upgrade cluster $KOPS_NAME\nITEM PROPERTY OLD NEW\nCluster KubernetesVersion 1.6.2 1.7.10\nInstanceGroup/master-eu-west-1a Image kope.io/k8s-1.6-debian-jessie-amd64-hvm-ebs-2017-05-02 kope.io/k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-07-28\nInstanceGroup/master-eu-west-1b Image kope.io/k8s-1.6-debian-jessie-amd64-hvm-ebs-2017-05-02 kope.io/k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-07-28\nInstanceGroup/master-eu-west-1c Image kope.io/k8s-1.6-debian-jessie-amd64-hvm-ebs-2017-05-02 kope.io/k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-07-28\nInstanceGroup/nodes Image kope.io/k8s-1.6-debian-jessie-amd64-hvm-ebs-2017-05-02 kope.io/k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-07-28\nMust specify \u0026ndash;yes to perform upgrade\nVerify the options that Kops provides, and if satisfied, add --yes to the command for upgrading the cluster configuration.\n$ kops upgrade cluster $KOPS_NAME \u0026ndash;yes\nITEM PROPERTY OLD NEW\nCluster KubernetesVersion 1.6.2 1.7.10\nInstanceGroup/master-eu-west-1a Image kope.io/k8s-1.6-debian-jessie-amd64-hvm-ebs-2017-05-02 kope.io/k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-07-28\nInstanceGroup/master-eu-west-1b Image kope.io/k8s-1.6-debian-jessie-amd64-hvm-ebs-2017-05-02 kope.io/k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-07-28\nInstanceGroup/master-eu-west-1c Image kope.io/k8s-1.6-debian-jessie-amd64-hvm-ebs-2017-05-02 kope.io/k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-07-28\nInstanceGroup/nodes Image kope.io/k8s-1.6-debian-jessie-amd64-hvm-ebs-2017-05-02 kope.io/k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-07-28\nUpdates applied to configuration.\nYou can now apply these changes, using `kops update cluster $KOPS_NAME`\nNext, thing is to push this new configuration to AWS, with the update command.\n$ kops update cluster $KOPS_NAME\n\u0026hellip; a lot of output \u0026hellip;\nVerify the changes that Kops will make in AWS and when satisfied, accept by appending the --yes flag to the above command.\n$ kops update cluster $KOPS_NAME \u0026ndash;yes\nI1112 14:00:00.516806 4406 dns.go:91] Private DNS: skipping DNS validation\nI1112 14:00:01.500941 4406 executor.go:91] Tasks: 0 done / 103 total; 39 can run\nI1112 14:00:02.470532 4406 executor.go:91] Tasks: 39 done / 103 total; 20 can run\nI1112 14:00:03.252511 4406 executor.go:91] Tasks: 59 done / 103 total; 30 can run\nI1112 14:00:05.935267 4406 executor.go:91] Tasks: 89 done / 103 total; 8 can run\nI1112 14:00:06.095672 4406 dnsname.go:110] AliasTarget for \u0026ldquo;\u0026hellip;\u0026rdquo; is \u0026ldquo;\u0026hellip;..\u0026rdquo;\nI1112 14:00:06.481874 4406 executor.go:91] Tasks: 97 done / 103 total; 6 can run\nI1112 14:00:07.024391 4406 executor.go:91] Tasks: 103 done / 103 total; 0 can run\nI1112 14:00:07.024442 4406 dns.go:152] Pre-creating DNS records\nI1112 14:00:07.581470 4406 update_cluster.go:247] Exporting kubecfg for cluster\nKops has set your kubectl context to $KOPS_NAME\nCluster changes have been applied to the cloud.\nChanges may require instances to restart: kops rolling-update cluster\nRolling update your cluster Kops provides a couple of different options for rolling updating your cluster. The default behavior will stop your instance one by one with a default timeout until all nodes has been restarted and updated. Kops also provides a more safe rolling-update with the feature flag +DrainAndValidateRollingUpdate. This flag will first cordon the node, which will disable scheduling on the node. When this is done, Kops will drain the node which will give all pods running on the node a gracefully shutdown and rescheduling.\nexport KOPS_FEATURE_FLAGS=”+DrainAndValidateRollingUpdate”\nThis is a great feature, however it could potentially cause downtime while updating your cluster. Kops will shutdown a node before spinning up a new one. Depending on your resource capacity this may be an issue. If that’s the case, consider scaling the cluster before a production upgrade.\nAnother problem with this approach is draining the code, which will shutdown all pods on the instance, potentially resulting in bottlenecks on the nodes where pods will be rescheduled because of image download time. Further there’s no priority of which pods while be restarted first, meaning kube-system pods, such as kube-dns may be the last pod to get downloaded and restarted.\nInstead of using these approaches, I’ve been doing it in a more manual fashion to insure no downtime during production upgrades.\nTo continue our previous example, let’s start out by performing a rolling-updating on our master nodes, one by one.\n$ kops rolling-update cluster $KOPS_NAME \u0026ndash;instance-group master-eu-west-1a \u0026ndash;yes\n$ kops rolling-update cluster $KOPS_NAME \u0026ndash;instance-group master-eu-west-1b \u0026ndash;yes\n$ kops rolling-update cluster $KOPS_NAME \u0026ndash;instance-group master-eu-west-1c \u0026ndash;yes\nYou should be able to combine this to one command. However, I like to verify that everything is running before continuing.\n$ kubectl get nodes -L kubernetes.io/role\nNAME STATUS AGE VERSION ROLE\nip-172-20-115-44.eu-west-1.compute.internal Ready 1m v1.7.10 master\nip-172-20-127-107.eu-west-1.compute.internal Ready 29m v1.6.2 node\nip-172-20-41-196.eu-west-1.compute.internal Ready 19m v1.7.10 master\nip-172-20-63-108.eu-west-1.compute.internal Ready 28m v1.6.2 node\nip-172-20-69-180.eu-west-1.compute.internal Ready 29m v1.6.2 node\nip-172-20-89-246.eu-west-1.compute.internal Ready 6m v1.7.10 master\nNext, we are going to rolling update our nodes one by one.\nWe start out by making the node unschedulable:\n$ kubectl cordon Now, one by one, delete pods and wait for them to reschedule on another node.\n$ kubectl delete pod You can use the following command to list all pods running on the particular node\n$ kubectl get pods \u0026ndash;all-namespaces -owide | grep When all pods are moved and restartet, drain the node:\n$ kubectl drain \u0026ndash;force \u0026ndash;ignore-daemonsets Last thing, go to the AWS EC2 console and terminate the node.\nRepeat these steps for all worker nodes.\nWhen all worker nodes has been rolling updated, the cluster upgrade is finished, and you should see all nodes running Kubernetes 1.7.10.\n$ kubectl get nodes -L kubernetes.io/role\nNAME STATUS AGE VERSION ROLE\nip-172-20-115-44.eu-west-1.compute.internal Ready 10m v1.7.10 master\nip-172-20-117-194.eu-west-1.compute.internal Ready 1m v1.7.10 node\nip-172-20-41-196.eu-west-1.compute.internal Ready 28m v1.7.10 master\nip-172-20-42-10.eu-west-1.compute.internal Ready 5m v1.7.10 node\nip-172-20-67-240.eu-west-1.compute.internal Ready 3m v1.7.10 node\nip-172-20-89-246.eu-west-1.compute.internal Ready 15m v1.7.10 master\nWhen finished trying the upgrade procedure, you can easily delete all resources created by Kops:\n$ kops delete cluster $KOPS_NAME \u0026ndash;yes\nThat’s it. Happy upgrading your Kops clusters.\n","permalink":"/posts/2017-11-14_upgrading-a-ha-kubernetes-kops-cluster-9fb34c441333/","tags":null,"title":"Upgrading a HA Kubernetes Kops Cluster"},{"categories":["Conference"],"contents":"CloudNativeCon + KubeCon Europe 2017 is over! Wow, what a great conference. It was great to meet so many engaged and passionate people working the Cloud Native ecosystem.\nIn this post, we will try to give our perspective on the conference, by highlighting the key takeaways from the talks we attended.\nDay 1 Pancake and Podcast — The New Stack The first event of the day was the Pancake and Podcast event hosted by The New Stack. This was a panel debate about Continuous Integration/Continous Delivery. The panel consisted of the following: Kris Nova (Deis), Aparna Sinha (Google), Fintan Ryan (Redmonk), Aaron Rice (Wercker). The topic of the day was CI/CD and Kubernetes. Many people are moving towards the SaaS-based CI/CD solutions, but there is still a lot Jenkins, especially in the enterprise. The CI/CD as a Service solutions look really promising especially those that promise a smooth integration with Kubernetes. Unfortunately, from our experience, the maturity and the ability to customize many of these solutions is still not fully there. Kris Nova pointed out that we need to ask ourselves as a community WHAT we want instead of focussing on HOW.\nMorning Keynotes The morning keynotes was an action-packed affair, with a total of 8 lightning keynotes. First, Executive Director of the Cloud Native Computing Foundation, Dan Kohn, took the stage and officially announced that the rkt and containerd projects now are hosted by the CNCF. This is a pretty significant step, that hopefully will put the discussions revolving around the docker fork to rest, and bring some stability and optionality in choosing the container runtime. The following 2 keynotes were; an introduction to containerd by Patrick Chanezon, Member of the Technical Staff at Docker Inc; and an introduction to rkt, By Brandon Philips, CTO of CoreOS.\nNext up was Product Management Team Lead of Google, Aparna Sinha, giving the highlights of the Kubernetes 1.6 release. If you want to know more, please check out the official blog post of the new features in 1.6 here. Just a brief comment to the 1.6 release, great job guys! Really pleased to see RBAC getting so much attention.\nThe last keynotes featured great talks about the OpenAI project, Kubernetes/Container Security, the history of Prometheus along with the last year of the Project Calico.\nRunning workloads in Kubernetes Janet Kuo, Software Engineer, Google Janet introduced four different workload patterns in Kubernetes: stateless, stateful, daemon, and batch, and gave examples of how e.g. web servers, databases, and log collectors fit these patterns. Afterward, she explained how the patterns map to Kubernetes’ concepts of deployments, stateful sets, daemon sets, and jobs. A well-planned demonstration of each pattern with its matching Kubernetes concept was given. All-in-all, a great introduction to the concepts of Kubernetes. Check of the slides for more information.\nCounting with Prometheus Brian Brazil, Robust Perception Brian’s talk was focused around how you should use counters in Prometheus. The dos and don’ts. One of the misconceptions that were pointed out is that the delta function should only be used with gauges and not counters, instead, you should use the irate function. Brian further highlighted some of the misconceptions around monitoring with Prometheus, it is not real-time data points, but aggregated data over a period of time. There are differences in time from when Prometheus scrapes your system and when the actual event took place, how the scraped data is collected and aggregated. Prometheus is meant for spotting symptoms. If you want complete stats, you should use logs instead, or as a compliment.\nWhen Failure is Not an Option: Processing Real Money at Monzo with Kubernetes and Linkerd Oliver Gouild, Buoyant \u0026amp; Oliver Beattie, Monzo Oliver Beattie started out by explaining what Monzo is and motivated the rest of the talk by highlighting some of the important characteristics of their software architecture such as extensibility, efficiency, resilience, and security. Oliver Gouild did the main part of the talk by presenting how linkerd helps to achieve these goals and how it is done under the hood. He covered how linkerd implements intelligent layer 5 load balancing using exponentially-weighted moving average (EWMA) and the importance of backpressure among other things.\nDance Madly on the lip of a Volcano with Security Release Processes Jess Frazelle, Google \u0026amp; Brandon Philips, CoreOS The focus of this talk by Jess \u0026amp; Brandon covered the new security release process for Kubernetes and all it’s sub-projects. The key take-aways was greatly summarized by Brandon Philips slide describing the audience homework: Join Kubernetes Announce, Review the Kubernetes Security Process, and help improve. If you are using Kubernetes, do yourself a favor and checkout the links above.\nOperational and Business Insights from Tracing Distributed Microservices Juraci Paixaão Kröhling, Red Hat Juraci started out by describing the differences between traditional and distributed tracing and gave a brief introduction to OpenTracing. Hawkular, an open source implementation of the OpenTracing standard, was the topic of the rest of the talk. Juraci started an example of an order management system using Hawkular APM and generated orders in order to see the data and graphical representation of the business transactions from start to end. The representation was shown in OpenShift Origin. Additionally, Juraci showed how the OpenTracing spans were used in the different Java services.\nKubernetes Operators: Managing Complex Software with Software Josh Wood, CoreOS \u0026amp; Jesus Carrillo, Ticketmaster Josh started out this presentation with a great introduction of what a Kubernetes Operator is and why you should use them. An Operator makes it easy to deploy stateful apps to Kubernetes, and further, managing, scaling and updating these apps becomes very easy. The operator consists of a simple loop similar to the loop in the ReplicaSet keeping your desired state, by Observing, Analyzing, and taking Action to get to the desired configuration. CoreOS has already created operators for etcd and Prometheus, and other will come in the future. Ticketmaster uses the Prometheus operator, and Jesus spoke about their experiences running it in production and delivering it as Prometheus as a Service for internal teams. Great talk, if you are interested, slides can be found here.\nClosing Keynotes\nThe closing keynotes of day 1 consisted of a series of 6 small keynotes from: Chen Goldberg, Director of Engineering, Container Engine \u0026amp; Kubernetes, Google; Brandon Philips, CTO, CoreOS; Mark van Straten, Senior Software Architect, Q42; Nicholas Weaver, Director of Software Engineering, Data Center Solutions Group, Intel Corporation; Michelle Noorali, Software Engineer, Deis; Kelsey Hightower, Google Cloud Team \u0026amp; CloudNativeCon and KubeCon Conference Co-Chair.\nThe velocity of the Kubernetes project is insane! The community is extremely hardworking and dedicated to making Kubernetes the defacto standard (if it isn’t already) for running container package software.\nFurther, Google is no longer the majority committer to the Kubernetes project. The community is a diverse community consisting of big enterprises, small startups to individual committers.\nWow, Philips Hue is being controlled by Kubernetes if you use My Hue or any of the services that talks to the Hue Cloud API. Mark gave an interesting story on how they build their architecture. Further, Brandon Philips introduced and demoed a new offering by quay.io for storing helm charts.\nThe last keynote, before the closing remarks by the always great Kelsey Hightower, was Michelle Noorali. Michelle argued that Kubernetes still is hard for developers to use. She made a comparison with a broom, a vacuum cleaner, and a Roomba (robot vacuum cleaner). The broom and the vacuum cleaner being the more manual ways of doing operations work, whereas the Roomba represents what Kubernetes has done for the operations community — no more hard work — just sit back and watch the robot do the work. This is to some degree true for the operations side of Kubernetes. However, developers’ jobs have gotten harder, because now they need to learn to use kubectl, containers, etc. Developers want a Roomba too. Something like the Deis Workflow, where developers don’t have to know about all these things, but instead just git push deis master.\nAll in all, day 1 was pretty exiting. Great job by the organizers, even though many rooms were a bit crowded! And great job by the host of the day, Kelsey Hightower!\nDay 2 Morning keynotes The second day started out with a keynote by Alexis Richardson, CEO of Weaveworks and TOC Chair CNCF, about what Cloud Native is and why we should care about it. He accentuated three pillars: speed, freedom, and trust. The need for speed was motivated by Netflix’s historical needs and Weaveworks’ own needs. In CNCF speed is promoted e.g. by interoperability between projects and education. Trust is important when choosing a tool, and the CNCF plays an important role as seen with The Linux Foundation. Freedom is seen in the absence of vendor/cloud lock-in, but also in the option of choice between containerd and rkt. Alexis underlined that CNCF shouldn’t act as a kingmaker, but instead be open to choice as seen with containerd and rkt both being accepted as projects. Wrapping up these thoughts, Alexis extended Marc Andreessen’s original quote “Software Is Eating the World” to “Software Is Eating the World, Open Source is Eating Software, Cloud is Eating Open Source”\nJoe Beda, CTO Heptio, gave a great talk on how to grow the Kubernetes user base, reflected on other user segments’ viewpoints of Kubernetes, and were Kubernetes has room for improvement or inconsistencies.\nThe last morning keynote was given by Kelsey Hightower on Cluster Federation. Kelsey looked at when (and when not) to use Cluster Federation with a healthy criticism. Furthermore, he pointed out some parts of the current state that don’t seem ready yet. Anyway, he presented a setup of four clusters running in four different regions. A terminal in each region was launched, producing traffic to the clusters, and from GCP’s web console the traffic was seen spread across the clusters. As always, Kelsey completely nailed the demo despite the fear of the demo gods.\nBuilding for Trust: How to secure your Kubernetes Cluster Alexander Mohr \u0026amp; Jess Frazelle, Google Alexander and Jess introduced and covered several ways of improving the security in a Kubernetes cluster. The new RBAC in Kubernetes 1.6 was introduced, and they described how it provides support for multiple identities and what the future roadmap might look like. How to use AppArmor and seccomp annotations on pods was also shown, and Jess demonstrated how the Dirty COW privilege escalation vulnerability could be avoided by annotating pods. A cluster, in which one of three nodes was vulnerable to Dirty COW, was used to show that an attack was blocked when using seccomp annotation and that it wasn’t blocked on the vulnerable node without the annotation.\nKubernetes Day 2: Cluster Operations Brandon Philips, CoreOS Brandon focused on day 2 operations in this fast-paced talk. What happens if e.g. the scheduler fails? Or you accidently scale it to 0? Brandon demonstrated how easy you can get back up and running by exporting the schedulers declaration, change it from a deployment to a pod. He further spoke about the advantages of using Kubernetes Operators, and especially how the etcd operator should be scaled for HA. The last part of the talk Brandon introduced the Prometheus operator and how you can monitor your Kubernetes cluster. If you want to know more, check out this repo and the slides.\nAutoscaling a Multi-Platform Kubernetes Cluster Built with kubeadm Lucas Käldström, Upper Secondary School Student, Individual Maintainer If you haven’t heard about Lucas Käldström, I’m sure you will in the future! Lucas is only 17 years old and has been a Kubernetes maintainer for about a year. Before that, he built the project: kubernetesonarm, that made setting up a Kubernetes cluster on arm easy! We used this project as the base for our Kubernetes Raspberry Pi clusters. His talk was a demonstration of how he has built a multi-architecture kubernetes cluster using kubeadm among other tools to set up the cluster. With this cluster, Lucas further demonstrated how to use custom metrics for the Horizontal Pod Autoscaling feature of Kubernetes. We highly recommend you to check out his workshop explaining all the nitty gritty details of the setup here.\nDelve into Helm: Advanced DevOps Lachlan Evenson \u0026amp; Adam Reese, Deis The high spirit of this talk was impossible not to notice with all the jokes about YAML indenting. They briefly covered how Helm can be used as a package manager using charts, and went on to what else Helm can offer. You can e.g. find best practices for running popular applications from kubeapps and do lifecycle management such as update, rollback, config management, and testing. In combination with an awesome demonstration of an example project: croc hunter, this worked really well. Especially the option to run helm test to verify that a chart is created as expected seems like a feature worth looking into. If your interest has been awaken, take a look at the slides.\nThe Patterns of Distributed Logging and Containers Satoshi Tagomori, Treasure Data Inc. Satoshi, who is a core team member of fluentd, spoke about the patterns of distributed logging. The first part of the talk focused on the different possibilities for picking up logs when working with docker containers, and how to transform these logs into structured data. Next, Satoshi focused on how to scale a logging infrastructure, by discussing the different patterns that are available in terms of how log aggregation should be handled. We highly recommend you to go through his slides, which can be found here.\nAll Attendees Party The conference ended with a great party sponsored by CoreOS, with great food and lots of great conversation.\nAll in all — CloudNativeCon + KubeCon has been an awesome conference! We’ve met some really cool people! Everybody has been very welcoming and happy to share their interests. Great talks, great venue — even though the rooms were a bit crowed sometimes.\nThis is definitely not the last time we attend a CloudNativeCon + KubeCon Conference!\n… and hey — next CloudNativeCon + KubeCon Europe will be in our home country of Denmark! See you in 2018!\n/Martin \u0026amp; Kasper\nOriginally published at kubecloud.io on April 2, 2017.\n","permalink":"/posts/2017-04-02_cloudnativecon-kubecon-europe-2017-12c37c3af38c/","tags":null,"title":"CloudNativeCon + KubeCon Europe 2017"},{"categories":["Kops"],"contents":"In the first post of our mini-series about Kops, I demonstrated how you could set up a highly available Kubernetes cluster on AWS in an existing VPC leveraging the awesome open-source project Kops.\nIn order to demonstrate how this could be accomplished, I demonstrated how to set up all the networking, with VPC, subnets, route tables, etc. This may have skewed people’s understanding of the ease of use of Kops. This was not the intention.\nHowever, I think the post demonstrated an actual use-case that many people migrating to Kubernetes on AWS is facing. At least it was the requirements I was facing in our migration path towards Kubernetes. We already had multiple services running in a VPC, like RDS databases, VPN connections, etc. The Kubernetes cluster, therefore, had to be spun up within this VPC in order to be able to communicate with the already existing services using private IPs.\nTherefore, to show some of the magic that Kops can do, I will write this follow-up post showing you how fast you can get up and running with a highly available Kops cluster similar to the previous post.\nAs in the previous post, we want a highly available cluster spread across multiple availability zones, along with private networking to ensure a closed environment. The following diagram illustrates the setup we would like to accomplish.\nThere are a couple of prerequisites that need to be in place before we can spin up our cluster. First, you need an AWS Account, a domain, Kops, awscli (not necessary since you can do the same in the console).\nIn this post, I will assume that you already have a perfectly working AWS account and that you have configured Route53 to handle DNS for your domain. (if this is not the case, check out the official documentation of kops here.)\nGreat! Let’s get going.\nThe first thing you have to do is to set up a bucket that Kops will use to store the cluster state. I will create this bucket using the awscli tool, but feel free to create it using the console.\n$ aws s3api create-bucket --bucket kubecloud-phennex-state-store --region eu-west-1 You should use versioning, especially when running in production. This enables you to revert you state to an earlier version.\n$ aws s3api put-bucket-versioning --bucket kubecloud-phennex-state-store --versioning-configuration Status=Enabled In the following I will be using Kops 1.5.3. (You can download it here)\nLets create the HA Cluster with Kops…\nYeah! Now we are ready to create our Highly Available Kubernetes cluster. Awesome! Let’s make things a bit easier by storing some of our configuration in ENV variables.\nexport NAME=\u0026lt;CLUSTER_NAME\u0026gt; export KOPS_STATE_STORE=s3://\u0026lt;BUCKET_NAME\u0026gt; export DNS_ZONE_PRIVATE_ID=\u0026lt;ID_OF_PRIVATE_HOSTED_ZONE\u0026gt; Let’s use Kops to create the cluster.\nkops create cluster \\ --node-count 3 \\ --zones eu-west-1a,eu-west-1b,eu-west-1c \\ --master-zones eu-west-1a,eu-west-1b,eu-west-1c \\ --dns-zone=${DNS_ZONE_PRIVATE_ID} \\ --dns private \\ --node-size t2.medium \\ --master-size t2.small \\ --topology private \\ --networking weave \\ --bastion \\ ${NAME} --node-count defines the number of nodes. I chose the node-count to be 3.\n--zones and --master-zones defines the zones we would like to span. As described earlier we chose; eu-west-1a, eu-west-1b, eu-west-1c. Both for the masters and the worker nodes.\n--dns-zones the id of the private zone to use.\n--dns private defines that we want to use a private hosted zone.\n--node-size and --master-size defines the instance types we want yo use.\n--topology private specifies that we want our nodes to be in the private subnets.\n--networking weave defines the network to be used. Select the network you want to use. See available options at this link.\n--bastion tells Kops that we want a bastion jump server in our cluster. This allows us to inspect the nodes via SSH.\nBe aware, that Kubernetes recommends running m3 instances for production usage. However, for demonstration purpose I will use cheaper instances.\nYou can always dry-run your cluster configuration with:\nkops update cluster ${NAME} This command will output what Kops intent to do, and you should verify that everything looks as expected. You can always edit this configuration with kops edit cluster ${NAME}\nOnce, you are ready to go, run the following command:\nkops update cluster ${NAME} --yes You should see something similar to this:\n$ kops update cluster ${NAME} --yes I0312 19:22:21.016372 15912 dns.go:90] Private DNS: skipping DNS validation I0312 19:22:23.297797 15912 executor.go:91] Tasks: 0 done / 112 total; 34 can run I0312 19:22:24.297299 15912 vfs_castore.go:422] Issuing new certificate: \u0026quot;kubecfg\u0026quot; I0312 19:22:24.386945 15912 vfs_castore.go:422] Issuing new certificate: \u0026quot;master\u0026quot; I0312 19:22:24.824109 15912 vfs_castore.go:422] Issuing new certificate: \u0026quot;kubelet\u0026quot; I0312 19:22:26.683630 15912 executor.go:91] Tasks: 34 done / 112 total; 27 can run I0312 19:22:29.562652 15912 executor.go:91] Tasks: 61 done / 112 total; 34 can run I0312 19:22:30.127307 15912 launchconfiguration.go:310] waiting for IAM instance profile \u0026quot;bastions.kubecloud.phennex.com\u0026quot; to be ready I0312 19:22:30.693262 15912 launchconfiguration.go:310] waiting for IAM instance profile \u0026quot;masters.kubecloud.phennex.com\u0026quot; to be ready I0312 19:22:30.903973 15912 launchconfiguration.go:310] waiting for IAM instance profile \u0026quot;masters.kubecloud.phennex.com\u0026quot; to be ready I0312 19:22:30.915987 15912 launchconfiguration.go:310] waiting for IAM instance profile \u0026quot;nodes.kubecloud.phennex.com\u0026quot; to be ready I0312 19:22:31.019373 15912 launchconfiguration.go:310] waiting for IAM instance profile \u0026quot;masters.kubecloud.phennex.com\u0026quot; to be ready I0312 19:22:41.459082 15912 executor.go:91] Tasks: 95 done / 112 total; 10 can run I0312 19:22:42.347766 15912 executor.go:91] Tasks: 105 done / 112 total; 7 can run I0312 19:22:42.413622 15912 natgateway.go:266] Waiting for NAT Gateway \u0026quot;nat-0ae48b59d01aac34f\u0026quot; to be available (this often takes about 5 minutes) I0312 19:22:42.413821 15912 natgateway.go:266] Waiting for NAT Gateway \u0026quot;nat-0b16ed2fe55e3b300\u0026quot; to be available (this often takes about 5 minutes) I0312 19:22:42.482413 15912 natgateway.go:266] Waiting for NAT Gateway \u0026quot;nat-0858a6417ddca6f17\u0026quot; to be available (this often takes about 5 minutes) I0312 19:24:43.871503 15912 executor.go:91] Tasks: 112 done / 112 total; 0 can run I0312 19:24:43.871556 15912 dns.go:141] Pre-creating DNS records I0312 19:24:45.673919 15912 update_cluster.go:208] Exporting kubecfg for cluster Kops has set your kubectl context to kubecloud.phennex.com Cluster is starting. It should be ready in a few minutes. Suggestions: * validate cluster: kops validate cluster * list nodes: kubectl get nodes --show-labels * ssh to the bastion: ssh -i ~/.ssh/id_rsa admin@bastion.kubecloud.phennex.com * read about installing addons: https://github.com/kubernetes/kops/blob/master/docs/addons.md As you can see from the output, Kops will create all the needed resources at AWS including NAT Gateways for our private instances to be able to reach the public internet, the VPC, etc.\nThe creation of a cluster usally takes about 5–10 minutes.\nAs, also mentioned in the previous post, this will create all DNS entries in the private zone, thereby only making it accessible from within the AWS VPC. Therefore if you want to be able to connect to your cluster from your local machine, recreate the following entries in the public zone for your domain:\nTo verify that your cluster is up and running and reachable from your machine:\n$ kubectl get nodes NAME STATUS AGE ip-172-20-110-125.eu-west-1.compute.internal Ready,master 2m ip-172-20-110-3.eu-west-1.compute.internal Ready 2m ip-172-20-53-117.eu-west-1.compute.internal Ready,master 3m ip-172-20-54-164.eu-west-1.compute.internal Ready 2m ip-172-20-65-18.eu-west-1.compute.internal Ready 2m ip-172-20-87-33.eu-west-1.compute.internal Ready,master 4m Awesome, everything works as expected!\nThis is how easy it is to setup a highly available Kubernetes cluster on AWS using Kops! Kops just created all the network, the VPC, the NAT Gateway, the RouteTables for us!\nOriginally published at kubecloud.io on March 12, 2017.\n","permalink":"/posts/2017-03-12_ha-kubernetes-cluster-on-aws-kops-makes-it-easy-2337806d0311/","tags":null,"title":"HA Kubernetes Cluster on AWS? — Kops makes it easy"},{"categories":["Kops"],"contents":"This is the first post in a series of Kubernetes and Kubernetes Operations (Kops) related blog posts.\nThe purpose of this series is to provide a guide to setting up a production ready Kubernetes environment on AWS. There exists lots of options to accomplish this; among others, Techtonic, Kismatic, StackPoint Cloud, OpenShift, etc. In this post we will have a look at the open source tool; Kubernetes Operations — also called Kops. Kops seems to have gained a lot of traction, and the community around the project is very active. Thanks to all maintainers for their hard work!\nPrerequisites In order to follow along with this guide, you need a couple of tools and an AWS account.\n An AWS account and cli Kops installed on you machine v1.5.1 kubectl  This tutorial is tested on macOS. I don’t know whether there exists Windows support for Kops, yet.\nBefore we get our hands dirty, let’s draw up the setup we would like to build. Since the closest AWS Region to my location is Ireland, I will continue this tutorial with eu-west-1. You can, of course, choose the region closest to your geographic location. If you are not familiar with AWS, and especially AWS Networking, I recommend you to read up on the basics of this topic. Terms you will need knowledge about are:\n Virtual Private Cloud Subnets Route Tables Internet Gateway NAT Gateway  I will try to explain these terms vaguely going through the tutorial.\nAlright, this is what we want to accomplish.\nFirst, we want to create a VPC with a CIDR block of our choice. I chose to use 10.0.0.0/16. Read the guidelines for defining CIDR blocks on AWS before choosing.\nWithin this VPC, I want all my nodes to be placed on a private network without any Public IPs. This limits the surface area of attack by not making the nodes reachable from the Internet directly.\nFurther, I want to make this setup production ready. I want nodes spread across availability zones (physically located AWS datacenters) to make sure the Kubernetes cluster will survive a disturbance in one of the zones. I use the 3 available zones in eu-west, called eu-west-1a, eu-west-1b, eu-west-1c.\nEven though we don’t want our instances to be reachable from the public internet, we need the private instances to be able to reach the internet. In order to accomplish this, we have to create 3 NAT Gateway nodes; one for each private zone. The NAT Gateway allows private instances to communicate with the internet.\nThis tutorial can be used as a reference guide to how you can use Kops with an already set up VPC with subnets, NAT Gateways, etc. Before I can show you how you can use Kops with an existing setup, we need our own setup. Let’s set up the networking in AWS.\nSetting up the VPC in AWS If you haven’t already installed the awscli tool, go ahead and do it now. We will use the awscli tool extensively to create our network.\nIf you have multiple keys defined for the awscli tool in ~/.aws/credentials, set the profile for the AWS account you want to use.\n$ export AWS_DEFAULT_PROFILE=kni_private or, just run the aws configure.\nLet’s create the VPC. As mentioned earlier and shown in the diagram, I chose the cidr-block 10.0.0.0/16.\n$ aws ec2 create-vpc --cidr-block 10.0.0.0/16 --region eu-west-1 { \u0026quot;Vpc\u0026quot;: { \u0026quot;VpcId\u0026quot;: \u0026quot;vpc-a55e77c1\u0026quot;, \u0026quot;InstanceTenancy\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;Tags\u0026quot;: [], \u0026quot;State\u0026quot;: \u0026quot;pending\u0026quot;, \u0026quot;DhcpOptionsId\u0026quot;: \u0026quot;dopt-b8ee9cdd\u0026quot;, \u0026quot;CidrBlock\u0026quot;: \u0026quot;10.0.0.0/16\u0026quot;, \u0026quot;IsDefault\u0026quot;: false } } Modify the VPC to allow DNS hostnames by running the following command:\n$ aws ec2 modify-vpc-attribute --vpc-id \u0026lt;VPC_ID\u0026gt; --enable-dns-hostnames \u0026quot;{\\\u0026quot;Value\\\u0026quot;:true}\u0026quot; --region eu-west-1 Next, we need an Internet Gateway, let’s go ahead and create one.\n$ aws ec2 create-internet-gateway --region eu-west-1 The last thing, we need to configure in terms of the VPC and Internet Gateway is to attach the internet gateway to the VPC.\n$ aws ec2 attach-internet-gateway --internet-gateway-id \u0026lt;INTERNET_GATEWAY_ID\u0026gt; --vpc-id \u0026lt;VPC_ID\u0026gt; --region eu-west-1 Now, the VPC is set up. Next up is configuring the subnets we want to use.\nAs depicted in the diagram, I want 3 public subnets and 3 private subnets. Meaning a public and private subnet in each Availability Zone. Let’s go ahead and start with the public subnets.\nPublic subnets\nFor each of the three public zones repeat the following two commands. Remember to use the selected CIDR-block for each zone. In this example, we will use; 10.0.0.0/20 for the subnet in eu-west-1a; 10.0.16.0/20 for the subnet in eu-west-1b; 10.0.32.0/20 for the subnet in eu-west-1c.\n$ aws ec2 create-subnet --vpc-id \u0026lt;VPC_ID\u0026gt; --cidr-block \u0026lt;CIDR_BLOCK\u0026gt; --availability-zone \u0026lt;AVAILABILITY_ZONE\u0026gt; --region eu-west-1 Further, we want the public subnets to auto-assign public ip to instances. Let’s modify the subnet.\n$ aws ec2 modify-subnet-attribute --subnet-id \u0026lt;SUBNET_ID\u0026gt; --map-public-ip-on-launch --region eu-west-1 Repeat this for the rest of the public subnets.\nPrivate subnets\nLike before, we need 3 subnets. Let’s just go ahead and create them. The private subnets we will use are; 10.0.48.0/20 for eu-west-1a; 10.0.64.0/20 for eu-west-1b; 10.0.80.0/20 for eu-west-1c.\n$ aws ec2 create-subnet --vpc-id \u0026lt;VPC_ID\u0026gt; --cidr-block \u0026lt;CIDR_BLOCK\u0026gt; --availability-zone \u0026lt;AVAILABILITY_ZONE\u0026gt; --region eu-west-1 Again, repeat this for all the private subnets. Great! Now, we have 6 subnets, 3 private, and 3 public. Let’s move on by making the private subnets able to connect to the internet.\nSetting up the NAT Gateways in AWS We will, as described earlier, create 3 NAT Gateways. Since these NAT Gateways should connect to the public Internet, we also need to allocate 3 Elastic IPs.\n$ aws ec2 allocate-address --domain vpc --region eu-west-1 Use the AllocationId to create the NAT Gateway for the public zone in eu-west-1a:\n$ aws ec2 create-nat-gateway --subnet-id \u0026lt;SUBNET_ID\u0026gt; --allocation-id \u0026lt;ALLOCATION_ID\u0026gt; --region eu-west-1 Do the same for the 2 other public subnets.\nWe are almost there, the last thing on our agenda is to define the routes in our networking. Let’s go ahead and configure the Route Tables.\nConfiguring the Route Tables Let’s start with the Route Table for the 3 public zones. We can join the routes for the three public subnets in one Route Table and then associate the three public subnets to this Route Table.\nPublic Route Table\nFirst, we create the Route Table.\n$ aws ec2 create-route-table --vpc-id \u0026lt;VPC_ID\u0026gt; --region eu-west-1 Next, we create a route for the Internet Gateway, we previously created. Use the outputted Route Table ID as follows.\n$ aws ec2 create-route --route-table-id \u0026lt;ROUTE_TABLE_ID\u0026gt; --destination-cidr-block 0.0.0.0/0 --gateway-id \u0026lt;INTERNET_GATEWAY_ID\u0026gt; --region eu-west-1 Lastly, let’s associate our public subnets with the Route Table.\n$ aws ec2 associate-route-table --route-table-id \u0026lt;ROUTE_TABLE_ID\u0026gt; --subnet-id \u0026lt;SUBNET_ID\u0026gt; --region eu-west-1 Repeat this last step for the 3 public subnets.\nPrivate Route Tables\nNext, we have to create a Route Table for each of our 3 private zones.\n$ aws ec2 create-route-table --vpc-id \u0026lt;VPC_ID\u0026gt; --region eu-west-1 Create the route that points to our NAT Gateway.\n$ aws ec2 create-route --route-table-id \u0026lt;ROUTE_TABLE_ID\u0026gt; --destination-cidr-block 0.0.0.0/0 --nat-gateway-id \u0026lt;NAT_GATEWAY_ID\u0026gt; --region eu-west-1 Associate the subnet (be sure to connect the right subnet matching the availability zone of the NAT Gateway)\n$ aws ec2 associate-route-table --route-table-id \u0026lt;ROUTE_TABLE_ID\u0026gt; --subnet-id \u0026lt;SUBNET_ID\u0026gt; --region eu-west-1 Repeat the process for the last 2 private subnets as well.\nGreat! Now the initial AWS Networking is in place, and we are ready to go on!\nSetting up DNS in AWS Route53 I had already configured my AWS Route53 with my domain as a public hosted zone. If you haven’t you can follow the guide provided by Kris Nova at Link.\nYou need to set up a public hosted zone with your domain, and have your provider point to AWS DNS name servers. You can verify that this is the case by calling the following, and verify that the AWS DNS nameservers are responding.\n$ dig phennex.com NS ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.8.3-P1 \u0026lt;\u0026lt;\u0026gt;\u0026gt; phennex.com NS ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 39791 ;; flags: qr rd ra; QUERY: 1, ANSWER: 4, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;phennex.com. IN NS ;; ANSWER SECTION: phennex.com. 86399 IN NS ns-1252.awsdns-28.org. phennex.com. 86399 IN NS ns-1738.awsdns-25.co.uk. phennex.com. 86399 IN NS ns-251.awsdns-31.com. phennex.com. 86399 IN NS ns-822.awsdns-38.net. ;; Query time: 106 msec ;; SERVER: 8.8.8.8#53(8.8.8.8) ;; WHEN: Sun Jan 29 20:47:20 2017 ;; MSG SIZE rcvd: 166 You also have to set up a private hosted zone for your domain. Again, this was already setup in my account. The easy way to do this is by going to the Route53 in the web-console. Press Create Hosted Zone, enter the same domain as the public zone, and choose the type to be private and associate your VPC from the dropdown-list.\nCreating an S3 Bucket as Kops state store We need a place to store our cluster configuration, and S3 is the place that Kops uses. Create a bucket. Choose a name for the bucket. I use: --bucket kubecloud-state-store.\n$ aws s3api create-bucket --bucket kubecloud-state-store --region eu-west-1 Installing Kops Private networking and setting up a cluster in an existing VPC just entered the stable channel. Therefore, go ahead and fetch Kops 1.5.1. Kops Releases\nDownload the darwin binary and move it to your PATH.\n$ mv kops-darwin-amd64 /usr/local/bin/kops $ chmod +x /usr/local/bin/kops Verify that the version is installed correctly\n$ kops version Version 1.5.1 (git-01deca8) Create the HA Cluster with Kops\nYeah! Now we are ready to create our High Available Kubernetes cluster. Awesome! Let’s make things a bit easier by storing some of our configuration in ENV variables.\nexport NAME=\u0026lt;CLUSTER_NAME\u0026gt; export KOPS_STATE_STORE=s3://\u0026lt;BUCKET_NAME\u0026gt; export VPC_ID=\u0026lt;VPC_ID\u0026gt; export DNS_ZONE_PRIVATE_ID=\u0026lt;ID_OF_PRIVATE_HOSTED_ZONE\u0026gt; Let’s use Kops to create the cluster.\nkops create cluster \\ --node-count 3 \\ --zones eu-west-1a,eu-west-1b,eu-west-1c \\ --master-zones eu-west-1a,eu-west-1b,eu-west-1c \\ --dns-zone=${DNS_ZONE_PRIVATE_ID} \\ --dns private \\ --node-size t2.medium \\ --master-size t2.small \\ --topology private \\ --networking weave \\ --vpc=${VPC_ID} \\ --bastion \\ ${NAME} --node-count defines the number of nodes. I chose the node-count to be 3.\n--zones and --master-zones defines the zones we would like to span. As described earlier we chose; eu-west-1a, eu-west-1b, eu-west-1c. Both for the masters and the worker nodes.\n--dns-zones the id of the private zone to use.\n--dns private defines that we want to use a private hosted zone.\n--node-size and --master-size defines the instance types we want yo use.\n--topology private specifies that we want our nodes to be in the private subnets.\n--networking weave defines the network to be used. Select the network you want to use. See available options at this link.\n--vpc the vpc id to use.\n--bastion tells Kops that we want a bastion jump server in our cluster. This allows us to inspect the nodes via SSH.\nEdit the cluster\nEdit the cluster to match the networking we setup before.\n$ kops edit cluster ${NAME} Change the subnet configuration as follows:\nsubnets: - id: subnet-0e89f256 egress: nat-0b80a9c336c8e4e4b name: eu-west-1a type: Private zone: eu-west-1a - id: subnet-c3320ca7 egress: nat-0eed67fdd1d1a2b72 name: eu-west-1b type: Private zone: eu-west-1b - id: subnet-c09dafb6 egress: nat-0c297125cb75a6995 name: eu-west-1c type: Private zone: eu-west-1c - id: subnet-4d89f215 name: utility-eu-west-1a type: Utility zone: eu-west-1a - id: subnet-e4320c80 name: utility-eu-west-1b type: Utility zone: eu-west-1b - id: subnet-e09daf96 name: utility-eu-west-1c type: Utility zone: eu-west-1c Now, run kops update cluster ${NAME}. You should see output similar to the following:\n... ... Will modify resources: VPC/kubecloud.phennex.com Name \u0026lt;nil\u0026gt; -\u0026gt; kubecloud.phennex.com InternetGateway/kubecloud.phennex.com Name \u0026lt;nil\u0026gt; -\u0026gt; kubecloud.phennex.com Subnet/utility-eu-west-1c.kubecloud.phennex.com Name \u0026lt;nil\u0026gt; -\u0026gt; utility-eu-west-1c.kubecloud.phennex.com Subnet/eu-west-1b.kubecloud.phennex.com Name \u0026lt;nil\u0026gt; -\u0026gt; eu-west-1b.kubecloud.phennex.com Subnet/utility-eu-west-1a.kubecloud.phennex.com Name \u0026lt;nil\u0026gt; -\u0026gt; utility-eu-west-1a.kubecloud.phennex.com Subnet/eu-west-1a.kubecloud.phennex.com Name \u0026lt;nil\u0026gt; -\u0026gt; eu-west-1a.kubecloud.phennex.com Subnet/utility-eu-west-1b.kubecloud.phennex.com Name \u0026lt;nil\u0026gt; -\u0026gt; utility-eu-west-1b.kubecloud.phennex.com Subnet/eu-west-1c.kubecloud.phennex.com Name \u0026lt;nil\u0026gt; -\u0026gt; eu-west-1c.kubecloud.phennex.com Must specify --yes to apply changes When you are satisfied with the things that Kops wants to create, run the following. It’s a known issue that Kops tells that it will modify the name of the VPC, InternetGateway, etc. This will actually not happen.\n$ kops update cluster ${NAME} --yes Since, we specified the cluster to use a private hosted zone, the cluster will only be accessible from within AWS.\nWe therefore need to make a little workaround and use the public zone we created earlier.\nAs you can see, Kops has created some records in Route53 in the private zone.\nThe 2 records we want to be resolvable from the public internet are:\napi.kubecloud.phennex.com and bastion.kubecloud.phennex.com\nLet’s created them in the public zone instead. Go to the public zone, press Create Record Set enter: api.your_domain and choose an A-record, and tick Yes to Alias find the api loadbalancer in the list. Repeat for the bastion server as well.\nFinally delete the 2 records in the private zone.\nVerifying the cluster works We can now verify that we are able to connect to the cluster using kubectl\n$ kubectl get nodes NAME STATUS AGE ip-10-0-52-26.eu-west-1.compute.internal Ready,master 3m ip-10-0-56-14.eu-west-1.compute.internal Ready 2m ip-10-0-68-38.eu-west-1.compute.internal Ready 2m ip-10-0-76-65.eu-west-1.compute.internal Ready,master 3m ip-10-0-80-53.eu-west-1.compute.internal Ready,master 3m ip-10-0-94-201.eu-west-1.compute.internal Ready 2m SSH into the private nodes If you need access to you instances, you can always SSH into the bastion server and jump to all the instances from here.\n$ ssh-add ~/.ssh/id_rsa $ ssh -A admin@bastion.kubecloud.phennex.com $ ssh admin@\u0026lt;PRIVATE_IP_OF_INSTANCE\u0026gt; This was the first post in a series of blog posts about running Kubernetes on AWS using Kops.\nI hope you like it — happy hacking!\nOriginally published at kubecloud.io on January 30, 2017.\n","permalink":"/posts/2017-01-30_setting-up-a-highly-available-kubernetes-cluster-with-private-networking-on-aws-using-kops-65f7a94782ef/","tags":null,"title":"Setting up a Highly Available Kubernetes Cluster with private networking on AWS using Kops"},{"categories":["Raspberry Pi"],"contents":"It’s been a while since our last blog post. However, we are back and again working with Kubernetes on a daily basis. This blog post will show you how to set up a Kubernetes 1.4 cluster with HypriotOS 1.1.1 and the new kubeadm tool.\nPrerequisites  A couple of Raspberry Pis (minimum two) A switch to connect the Raspberry Pis Cables for power and network  For this tutorial we used 4 Raspberry Pi 3 and a Macbook Pro.\nFlashing the SD-cards First, flash the SD cards with HypriotOS 1.1.1 using the Hypriot flash tool.\n$ flash --hostname master hypriotos-rpi-v1.1.1./images.zip $ flash --hostname slave01 hypriotos-rpi-v1.1.1./images.zip $ flash --hostname slave02 hypriotos-rpi-v1.1.1./images.zip $ flash --hostname slave03 hypriotos-rpi-v1.1.1./images.zip When the flash of the 4 sd-cards has completed, insert them into the 4 Raspberry Pis and power them up. Make sure you are able to contact the Pis by SSH’ing into them one by one. The default password for the pirate-user is: hypriot\n$ ssh pirate@master.local $ ssh pirate@slave01.local $ ssh pirate@slave02.local $ ssh pirate@slave03.local After logging into the Pis, exit back to your machine.\nNext, we need to set up our SSH-keys:\nssh-add ssh-keygen -R master.local ssh-copy-id pirate@master.local Repeat the above for all the Pis.\nConfiguring the IPs To make things easier to handle, I want to change the IPs to static IPs. I chose the IP scheme to be:\n192.168.1.100 (master) 192.168.1.101 (slave01) 192.168.1.102 (slave02) 192.168.1.103 (slave03) Repeat for all Pis\nFirst SSH into the Pi\n$ ssh pirate@master.local $ sudo nano /etc/network/interfaces Replace the content of this file with\nauto lo iface lo inet loopback allow-hotplug eth0ifaceeth0 inet static address **\u0026lt;YOUR IP GOES HERE\u0026gt;** netmask 255.255.255.0 network 192.168.1.0 broadcast 192.168.1.255 gateway 192.168.1.1 dns-nameservers 192.168.1.1 8.8.8.8 8.8.4.4 iface eth0 inet6 auto allow-hotplug wlan0 iface wlan0 inet dhcp pre-up /usr/bin/occi wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf iface default inet dhcp Then reboot each device by running\nsudo reboot Installing Kubernetes Switch to the root user\nsudo -s And execute the following, which will add the Kubernetes apt-get respository to the resources.\n$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF Notice: Make sure there isn’t a space after EOF.\nUpdate everything\n$ apt-get update Then install the kubelet, kubeadm, kubectl, and kubernetes-cni\n$ apt-get install -y kubelet kubeadm kubectl kubernetes-cni Setting up the master kubeadm init --pod-network-cidr=10.244.0.0/16 The --pod-network-cidr=10.244.0.0/16 is needed for flannel to be configured correctly. Flannel is at the moment the only overlay network that works with Raspberry Pis. When the command has finished, (which will take a couple of minues) the output will be similar to the following:\nPublic: /etc/kubernetes/pki/ca-pub.pem Private: /etc/kubernetes/pki/ca-key.pem Cert: /etc/kubernetes/pki/ca.pem \u0026lt;master/pki\u0026gt; generated API Server key and certificate: Issuer: CN=kubernetes | Subject: CN=kube-apiserver | CA: false Not before: 2016-11-24 20:25:58 +0000 UTC Not After: 2017-11-24 20:26:02 +0000 UTC Alternate Names: [192.168.1.100 10.96.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] Public: /etc/kubernetes/pki/apiserver-pub.pem Private: /etc/kubernetes/pki/apiserver-key.pem Cert: /etc/kubernetes/pki/apiserver.pem \u0026lt;master/pki\u0026gt; generated Service Account Signing keys: Public: /etc/kubernetes/pki/sa-pub.pem Private: /etc/kubernetes/pki/sa-key.pem \u0026lt;master/pki\u0026gt; created keys and certificates in \u0026quot;/etc/kubernetes/pki\u0026quot; \u0026lt;util/kubeconfig\u0026gt; created \u0026quot;/etc/kubernetes/kubelet.conf\u0026quot; \u0026lt;util/kubeconfig\u0026gt; created \u0026quot;/etc/kubernetes/admin.conf\u0026quot; \u0026lt;master/apiclient\u0026gt; created API client configuration \u0026lt;master/apiclient\u0026gt; created API client, waiting for the control plane to become ready \u0026lt;master/apiclient\u0026gt; all control plane components are healthy after 188.931005 seconds \u0026lt;master/apiclient\u0026gt; waiting for at least one node to register and become ready \u0026lt;master/apiclient\u0026gt; first node is ready after 5.533117 seconds \u0026lt;master/apiclient\u0026gt; attempting a test deployment \u0026lt;master/apiclient\u0026gt; test deployment succeeded \u0026lt;master/discovery\u0026gt; created essential addon: kube-discovery, waiting for it to become ready \u0026lt;master/discovery\u0026gt; kube-discovery is ready after 185.528275 seconds \u0026lt;master/addons\u0026gt; created essential addon: kube-proxy \u0026lt;master/addons\u0026gt; created essential addon: kube-dns Kubernetes master initialised successfully! You can now join any number of machines by running the following on each node: kubeadm join --token=c3625d6ebda8defc 192.168.1.100 Should anything fail during the setup, run kubeadm reset, then systemctl start kubelet.service, and try again.\nNext, we need to install the cluster network, which as mentioned earlier will be flannel.\n$ export ARCH=arm $ curl -sSL \u0026quot;https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml?raw=true\u0026quot; | sed \u0026quot;s/amd64/${ARCH}/g\u0026quot; | kubectl create -f - Verify that the kube-dns pod is running: kubectl get pods --all-namespaces\nSetting up the slaves/workers Copy the command from the output of the master setup and run it on all your workers.\nkubeadm join --token \u0026lt;token\u0026gt; \u0026lt;master-ip\u0026gt; After a couple of minutes, your Raspberry Pi Kubernetes cluster is ready. Verify with kubectl get nodes on the master pi. The output should be similar to this:\nNAME STATUS AGE master Ready 13m slave01 Ready 1m slave02 Ready 51s slave03 Ready 30s Getting access to your cluster from you own machine Make sure you have kubectl installed. Then SSH into the master and open the following file:\n$ ssh pirate@master.local $ sudo cat /etc/kubernetes/admin.conf Copy the content of this file to a place on your own machine. I just copy/pasted it in to a file called raspberrypi.conf.\nNow, from your local machine you can access the cluster as follows:\n$ kubectl --kubeconfig raspberrypi.conf get nodes Now your Kubernetes cluster is ready to be used! Awesome! :)\nOriginally published at kubecloud.io on December 15, 2016.\n","permalink":"/posts/2016-12-15_installing-a-kubernetes-1-4-cluster-on-hypriot-1-1-1-with-kubeadm-b7495638610e/","tags":null,"title":"Installing a Kubernetes 1.4 cluster on Hypriot 1.1.1 with kubeadm"},{"categories":["Conference"],"contents":"The first European ContainerDays was held in Hamburg (Germany) on June 27 and 28, 2016. The first day consisted of workshops about Kubernetes, Docker (security), DC/OS, rkt, and monitoring. The second day consisted of presentation sessions held by 16 different speakers. We went on a trip from Aarhus to Hamburg to check out the second day’s sessions.\nFrom Borg to Kubernetes: The History and Future of Container Orchestration — Mandy Waite, Senior Staff Developer Advocate, Google Cloud Platform. Mandy described the evolution of Google’s internal cluster management systems, especially focusing on how the different systems work and resource scheduling. Internal experiments and lessons learned with Borg was presented in a condensed format. The high server utilization Google has achieved was presented, and among the important factors are bin packing and overcommitment of resources. The details behind Google’s internal experiments and lessons can be found in the Google white paper: Large-scale cluster management at Google with Borg. Furthermore, the evolution of different systems: Borg, Omega, and the open-source Kubernetes was described. Further details about this topic can be found in the Google white paper: Borg, Omega, and Kubernetes.\nEnterprise Microservices Adoption — Boyan Dimitrov, Sixt. One of the key takeaways from Boyan’s talk; ‘always do reality checks’. A great illustration of this is, engineers saying: “We need to build race cars” (which is the new cool thing of the week). When they finished building these new race cars, they realize they don’t have a track to drive the car — and they are forced to either throw away this awesome new car or build a brand new track. Therefore, always ask yourself, are we building the right thing? Boyan further described how they have adopted microservices and Continuous Delivery at Sixt. Technologies they used: HuBot, Jenkins, BitBucket, Docker, and Kubernetes.\nLinux kernel features building my $CONTAINER — Erkan Yanar, Freelancer. The focus of this talk was the deeper understanding of the Linux kernel features that container technology like Docker leverage. Erkan demonstrated how to use chroot, cgroups, and namespaces while describing what they make possible from a conceptional view. A fast-paced, entertaining, and educational tour of the underlying concepts behind Docker and Pods in Kubernetes.\nApplication Deployment and Management at Scale with 1\u0026amp;1 — Rainer Sträter, ..1\u0026amp;1. 1\u0026amp;1 are launching a new platform for hosted solutions built on Kubernetes. Rainer gave a thorough description of the advantages of moving from their own model following old Cloud Hosting principles.\nRancher Docker — From zero to hero — Michael Vogeler, Nexinto Gmbh. Unfortunately, we missed some of this talk, but it was a demonstration of how to setup a Kubernetes cluster using Rancher. It seemed really interesting, especially with the features of easily deploying a cluster as a ‘one click’ install whether it was a Docker Swarm, Kubernetes or Mesos cluster.\nPlan B: Service to Service Authentication with OAuth — Henning Jacobs, Zalando. Henning presented Zalando’s investigation of authentication between microservices and described Zalando’s strategy (Plan B) using OAuth 2.0 between different AWS accounts. At Zalando, autonomous teams are important which is reflected in their architecture as separate AWS accounts for teams. During the talk, considerations such as token revocation and decentralization were covered.\nLightning Talk — Florian Leibert, Founder/CEO, Mesosphere. The lightning talk by the founder of Mesosphere, Florian Leibert, started out with a rundown of Florian’s experiences with Mesos at Twitter and AirBnb which led to Mesosphere. An anecdote about Twitter’s previous scalability issues before Mesos was presented as the “Justin Bieber Problem”. Afterwards an introduction to DC/OS was given followed by a demonstration of a Twitter-like sample application called Tweeter. A similar demonstration of Mesos and DC/OS using Tweeter can be found here. Two great articles about Mesos are, Why the data center needs an operating system and Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center\nEfficient monitoring in modern environments — Tobias Schmidt, SoundCloud In this talk, Tobias presented his and SoundCloud’s best practices on monitoring from their experience. SoundCloud has developed and open sources their monitoring solution, Prometheus, which has become a Cloud Native Computing Foundation project. The talk consisted of four parts: monitoring, metrics, four golden signals, and alerting. Several thoughtful considerations were presented such as the risk of alert fatigue if alerts are triggered to easily when they aren’t important. Among other examples were the importance of runbooks/playbooks which are a concise description of a potential solutions when an alert is triggered. Several of the ideas overlap with what is described in the book: Site Reliability Engineering — How Google Runs Production Systems. Tobias’ slides contain more details e.g. on which metrics to collect.\nWe took a break during the last talks before the last keynote since we had a long way to drive to reach Aarhus before midnight. Unfortunately, the last keynote with Aaron Huslage from Docker was cancelled due to illness. All in all, it was a very interesting day with a lot of inputs from other’s experiences and considerations combined with interesting conversations with other sharing the same passion.\nOriginally published at kubecloud.io on July 13, 2016.\n","permalink":"/posts/2016-07-13_containerdays-hamburg-c4639abae4fa/","tags":null,"title":"ContainerDays Hamburg"},{"categories":["Thesis"],"contents":"It’s been a while since our last blog post. We have been crazy busy teaching a course about Microservices, Docker, Kubernetes, etc., using a Kubernetes Raspberry Pi cluster, named KubeCloud.\nDuring this course we have evaluated the use of KubeCloud and how it can help students understand these different concepts. We have a lot of material that we will be sharing after the defense of our thesis at June 20th.\nBut in the meantime, we have a little treat for you. We couldn’t help ourselves, and we had to build something special for our personal clusters.\nKubernetes Logo Shaped Cluster\nWe like the Kubernetes logo and thought it would be an awesome template for a cluster. See the images below:\nWhat you need to build your own Kubernetes shaped cluster\n 4pc Raspberry Pi (We used the Raspberry Pi 3 Model B) 4pc 16 GB MicroSDHC cards 1pc Small Switch (We used the d-link go-sw-5e) 4pc 0.3m Ethernet cables (we chose different colors for easy identification) 1pc USB Power Hub (We used Anker PowerPort 6 60W) 4pc Micro-USB cables 0.3m (approx 1ft)  The last thing you need is of course the rack it self. We had it custom made at a laser cutting company. An Illustrator template can be downloaded here.\nOnce your cluster is assembled, have a look at our previous guide on how to set up Kubernetes-on-ARM.\nStay tuned for more stuff about Kubernetes, Clusters, Raspberry Pis, Microservices, etc.\nOriginally published at kubecloud.io on June 12, 2016.\n","permalink":"/posts/2016-06-12_sneak-preview-934354cac84c/","tags":null,"title":"Sneak preview"},{"categories":["Thesis"],"contents":"This blog post will walk you through the steps of setting up kubernetes-on-arm that Luxas has created. Thanks to Luxas’ project it is pretty straightforward to get Kubernetes up and running on a Raspberry Pi. This guide will walk you through a fork of Luxas’ project, that contains minor adjustments to the DNS and a couple of extra scripts.\nOverall setup The goal of this guide to get a cluster of four nodes up and running. One master and three workers. In this guide ArchLinux will be installed on all of the nodes and static IPs assigned. We have tried ArchLinux out and found it most stable in our setup. Afterwards Kubernetes-on-arm will be installed, and each node configured as either a master or a worker. Let’s get started!\nSD cards First the SD cards must be flashed. Since we are using OSX, we used the build-in Disk Utility to erase each SD card with the format “MS-DOS (FAT)” and Scheme “GUID Partition Map”. Remember to unmount each card before you unplug it.\nAfterwards we continued on a Linux machine inserting one SD card at a time.\nsudo fdisk -l As seen above the SD card is found as /dev/sdb, your device might get a different name. To be able to write to the SD card /dev/sdb2 is unmounted. Afterwards pull kubernetes-on-arm and start writing with the arguments for the Raspberry Pi 2, ArchLinux and kubesystemd. There are more options on Luxas’ kubernetes-on-arm.\n$ sudo umount /dev/sdb2 $ git pull https://github.com/rpicloud/kubernetes-on-arm.git $ cd kubernetes-on-arm $ sudo sdcard/write.sh /dev/sdb rpi-2 archlinux kube-systemd You will be prompted that you will loose all data on the device, so make sure that you select the right device from the fdisk step.\nThe write script will download and copy the files to your SD card, and in the end you should see an output similar to the one below.\nRepeat this step for each SD card and you should be ready for the next step.\nStatic IP In order to know where the different nodes are both for yourself and Kubernetes, static IPs can be a help. Since we have eight clusters and a router with gateway IP 192.168.1.1, we use a convention that concatenates (notice || syntax later) the cluster number with the node number in the last section of the IP: 192.168.1.(cluster || node). Cluster 1 node 1 will get the IP: 192.168.1.11, while node 2 in the same cluster will get: 192.168.1.12.\nNow, how do we connect a Raspberry Pi? You can either plug in an HDMI-cable and a keyboard or use SSH. We plugged in one Raspberry Pi at a time, found the newly assigned IP under ‘logs’ section and changed it one at a time.\nAn example could be that your router assigned the first Raspberry Pi the IP 192.168.1.189 you can log in and change your IP in the following way.\n$ ssh root@192.168.1.189 # You will be asked if you trust the key fingerprint - type 'yes' # Afterwards type the password 'root' # Edit the following file e.g. with nano $ nano /etc/systemd/network/dns.network When you have opened the file, delete everything and paste in the lines below. Notice that 192.168.1.11 is the static IP address here. The DNS entry sets up 10.0.0.10 which is default for kube-dns, the local 192.168.1.1 gateway and Google's DNS.\n[Match] Name=eth0 [Network] Address=192.168.1.11/24 Gateway=192.168.1.1 Domains=default.svc.cluster.local svc.cluster.local cluster.local DNS=10.0.0.10 192.168.1.1 8.8.8.8 Hit ctrl+x and then ‘y’ to save the file and reboot to make the change go through.\nreboot Now connect as earlier with the new IP.\n$ ssh root@192.168.1.11 Installing Kubernetes (and Docker) From Luxas’ image comes a command line tool called kube-config that can control much of the configuration around Kubernetes. To install Docker and Kubernetes run:\nkube-config install It will take some time to download everything, and in the end you will be prompted for a name. We have used the same naming convention as the last section of the IP, giving cluster 1’s node 1 the name node11. Afterwards you will be asked for timezone, swapfile and if you want to reboot. The default values seems fine, so just press enter.\nSince one of your nodes must be a master, you have to branch out differently from here.\nMaster\nOn the master simply run the command:\nkube-config enable-master From the command has finished until Kubernetes is up can take some minutes. You can follow the creation of Docker containers by running docker ps and, at some point, kubectl get nodes. Get nodes will show the nodes connected to this master (including the master itself).\nSlave\nFor each slave (e.g. ending IP with 12, 13, 14) you need to wait for the master to be ready and then specify the ip of the master in the following way.\nkube-config enable-worker 192.168.1.11 When all of the nodes have downloaded and started a worker, check from the master that the workers are connected to it by running kubectl get nodes.\nStartup and shutdown The extra scripts, mentioned in the introduction, can help you start up and shut down your cluster from the master node without bringing the nodes in an inconsistent state. In order, for the master, to be able to run commands on the workers you need to set up ssh-keys and copy them to the workers. You can do so in the following way (e.g. from node11):\nssh-keygen # Hit enter about three times ssh-copy-id root@192.168.1.12 ssh-copy-id root@192.168.1.13 ssh-copy-id root@192.168.1.14 The scripts are placed in the /root/ folder, and you run them from the master node in the following way.\nsh startup.sh sh shutdown.sh Now you should be ready to play with Kubernetes on your Raspberry Pi cluster. Have fun with it!\nOriginally published at kubecloud.io on April 13, 2016.\n","permalink":"/posts/2016-04-13_-guide-setting-up-a-kubernetes-on-arm-cluster-on-raspberry-pis-baac1511be3d/","tags":null,"title":"[Guide] Setting up a Kubernetes on ARM cluster on Raspberry Pis"},{"categories":["Thesis"],"contents":"This guide will provide you with the stuff you need for putting a Raspberry Pi cluster together. The first thing you need is of course some Raspberry Pis . We choose to buy the Raspberry Pi 2 Model B, which is a pretty powerful little machine compared to the older Raspberry Pi. This little beast will hook you up with a quadcore 900 MHz processor and 1 GB of memory, which is descent for the price and the size.\nOur setup  4 Raspberry Pi 2 Model B 4 Raspberry Pi 2 power supplies (2A) D-link GO-SW-5E 4 RJ45 Cat5e Patch cables 4 16Gb Kingston Class 10 micro SDHC  The next thing we need are of course some power! There exist some pretty cool USB power supplys on the market, which offers the actual current needed for the Raspberry Pi (link). Notice that there are a big different of the requirements of the Raspberry Pi 2 compared to the older model. The requirements for the Raspberry Pi 2 is 1.8A opposite 1.2A for the older model. However we didn’t have the funds to buy USB power supply’s, so we bought the official white 2A power supply’s.\nNow we have some hardware and some power, but we are missing a vital piece. We need a memory card for storing our OS and all of our applications. It’s recommended that you buy a class 10 SDHC card or better. We decided to go with a Kingston 16GB micro SDHC. For installation of the distribution onto the micro SD card, please read our previous post.\nWe decided to create our own rack, if you don’t want do that, have a look at something like this. We designed the models in SolidWorks, and had them made a facility provided by Aarhus University. The design features holes for wiring, and room for the Switch. You can download the models here. The Zip-file contains 2 files, the top/bottom layer and the raspberry pi layers. We have 2 top/bottom and 4 raspberry pi layers in our cluster.\nHappy hacking! Over and out!\nOriginally published at kubecloud.io on February 11, 2016.\n","permalink":"/posts/2016-02-11_how-to-build-a-raspberry-pi-cluster-8af5f2f199d/","tags":null,"title":"How to build a Raspberry Pi Cluster"},{"categories":["Thesis"],"contents":"Welcome to our blog about the work we are about to begin within the subject of resilient cloud computing.\nThe title of our project is as follows: Resilient cloud computing in a controlled test environment\nAnd what does that involve? That is a really good question, and a question we will start a quest for the next five month to answer this. But for now, this title involves the following description:\n The focus of the present master’s thesis is twofold: i) to build a Raspberry Pi cluster as a fully-controlled small-scale cloud computing environment; and ii) to conduct practical, comparative, and theoretical investigations into fault tolerance, resilience, load balancing, and virtualization in cloud computing. Moreover, the cluster should be able to provide visual and/or textual feedback of relevant computing status information. All of the above should be designed, implemented, tested, and documented in a suitable, clear, and concise manner.\n So, we are going to look into 2 things, firstly how to build a Raspberry Pi cluster, and how can we build it so that its highly configurable. The idea is that our work is going to be embedded in an already existing course at Aarhus University School of Engineering, called Object-Oriented Network Communication. How can we build a cluster that works with fx. Kubernetes or Docker Swarm with 4 nodes? and how can this cluster be integrated with 7 identical clusters to form one big cluster without a lot of manual configuration?\nThats the initial quest of our journey. The second half is about designing a small scale model of a microservice setup. What does it mean to move your application into the world of modularized distributed systems, and how can you make it reliable? This involves testing out different solutions and design patterns, and investigating the overhead involved in the different ways of accomplishing the same thing.\nSo the title mentioned shopping tech! We have been so fortunate to get our hands on 32 Raspberry Pi 2 Model B and a lot cables, SD cards etc.\nSome images of our hardware\nLastly, to give you an impression of our idea of how the cluster is going to be designed physically, we created the following 3D model:\nHave a great day, and stay tuned for more exciting Raspberry Pi cluster stuff!\nOriginally published at kubecloud.io on January 25, 2016.\n","permalink":"/posts/2016-01-25_welcome-to-the-blog-and-shopping-tech/","tags":null,"title":"Welcome to the blog - and shopping tech!"}]