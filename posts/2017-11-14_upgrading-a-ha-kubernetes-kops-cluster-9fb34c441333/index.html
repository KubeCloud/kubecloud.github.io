<!doctype html><html lang=en-us><head><meta charset=utf-8><title>kubecloud</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="The Kubernetes release cadence is fast-paced with minor releases every quarter. Awesome! But how do I keep up?"><meta name=author content="kubecloud.io"><meta name=generator content="Hugo 0.62.2"><link rel=stylesheet href=https://kubecloud.github.io/website/plugins/bootstrap/bootstrap.min.css><link rel=stylesheet href=https://kubecloud.github.io/website/plugins/themify-icons/themify-icons.css><link rel=stylesheet href=https://kubecloud.github.io/website/scss/style.min.e92287ba30e877e203e37f813150a9226378acce473f0a198f39c3a0294241b4.css integrity="sha256-6SKHujDod+ID43+BMVCpImN4rM5HPwoZjznDoClCQbQ=" media=screen><link rel="shortcut icon" href=https://kubecloud.github.io/website/images/favicon.png type=image/x-icon><link rel=icon href=https://kubecloud.github.io/website/images/favicon.png type=image/x-icon></head><body><div class=preloader></div><header class=navigation><div class="col-md-10 offset-md-1"><nav class="navbar navbar-expand-lg navbar-white bg-transparent"><a class="navbar-brand mobile-view" href=https://kubecloud.github.io/website><img class=img-fluid src=https://kubecloud.github.io/website/images/kubecloud.png alt=kubecloud></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><a class="navbar-brand desktop-view" href=https://kubecloud.github.io/website><img class=img-fluid src=https://kubecloud.github.io/website/images/kubecloud.png alt=kubecloud></a><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=https://kubecloud.github.io/website>Home</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Blog</a><div class=dropdown-menu aria-labelledby=navbarDropdown><a class=dropdown-item href=https://kubecloud.github.io/website/blog>All</a><div class=dropdown-divider></div><a class=dropdown-item href=https://kubecloud.github.io/website/categories/conference>Conference</a>
<a class=dropdown-item href=https://kubecloud.github.io/website/categories/kops>Kops</a>
<a class=dropdown-item href=https://kubecloud.github.io/website/categories/raspberry-pi>Raspberry pi</a>
<a class=dropdown-item href=https://kubecloud.github.io/website/categories/thesis>Thesis</a></div></li><li class=nav-item><a class=nav-link href=https://kubecloud.github.io/website/about>About</a></li></ul><div class="search px-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://kubecloud.github.io/website/search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-10 mx-auto"><a href=https://kubecloud.github.io/website/categories/kops class=text-primary>Kops</a><h1>Upgrading a HA Kubernetes Kops Cluster</h1><div class="mb-3 post-meta"><span>By Kasper Nissen,</span>
<span>14 November 2017</span></div><img src=https://kubecloud.github.io/images/1__B5__q4EKPr6__d6YsYXtpovw.jpeg class="img-fluid w-100 mb-4" alt="Upgrading a HA Kubernetes Kops Cluster"><div class="content mb-5"><p><img src=https://kubecloud.github.io/website/images/1__B5__q4EKPr6__d6YsYXtpovw.jpeg alt></p><p>The Kubernetes release cadence is fast-paced with minor releases every quarter. Awesome! But how do I keep up?</p><p>Don’t worry, Kops makes it fairly easy to update your HA production cluster without any downtime (assuming you have scaled your deployments to a minimum of 2 pods per deployment).</p><p>This blog post will walk you through upgrading your Kubernetes cluster version 1.6.2 cluster to Kubernetes cluster 1.7.10. The following will (most-likely) be applicable with other versions as well. Before any production upgrade, some testing and walkthrough of the gameplan is always a good idea. We currently have three Kubernets clusters managed by Kops on AWS; dev, staging, and production. However, all environments are actively being used on a daily basis. Shutting down a cluster a day or two for testing is really not an option. Instead, Kops makes it incredibly easy to test your upgrade process by making it easy to spin up new clusters with specific configurations.</p><p>In the following section I will provide you with a hands-on description of how you can upgrade you cluster without any downtime.</p><p>Enough with the introduction, let’s get down to business!</p><p>Go to the Kops git-repo and download Kops version 1.6.2, or just click on this link: <a href=https://github.com/kubernetes/kops/releases/tag/1.6.2>https://github.com/kubernetes/kops/releases/tag/1.6.2</a></p><p>If you haven’t set up a Kubernetes Kops cluster before, I will advise you to go have a look at some of my previous posts about this subject. (Links: <a href=https://kubecloud.io/ha-kubernetes-cluster-on-aws-kops-makes-it-easy-2337806d0311>HA Kubernetes Cluster on AWS? — Kops makes it easy</a> and <a href=https://kubecloud.io/setting-up-a-highly-available-kubernetes-cluster-with-private-networking-on-aws-using-kops-65f7a94782ef>Setting up a Highly Available Kubernetes Cluster with private networking on AWS using Kops</a>)</p><p>In the following I assume that you have setup all the prerequisites for spinning up a Kops cluster, such as an S3 bucket, Route53, etc.</p><p>Let’s get started.</p><p>Set up your environment variables for the Kops, and configure you AWS profile (I’m using multiple AWS configs) as follows:</p><p>export KOPS_STATE_STORE=”s3://path_to_your_bucket”<br>export KOPS_NAME=name_of_your_new_cluster<br>export PRIVATE_HOSTED_ZONE_ID=id_of_your_route53_private_zone<br>export AWS_PROFILE=your_aws_profile</p><p>Spin up a new cluster with Kops 1.6.2 installed using the following configuration</p><p>kops create cluster \<br>&ndash;name $KOPS_NAME \<br>&ndash;state $KOPS_STATE_STORE \<br>&ndash;node-count 3 \<br>&ndash;zones eu-west-1a,eu-west-1b,eu-west-1c \<br>&ndash;master-zones eu-west-1a,eu-west-1b,eu-west-1c \<br>&ndash;dns-zone=${PRIVATE_HOSTED_ZONE_ID} \<br>&ndash;dns private \<br>&ndash;node-size t2.medium \<br>&ndash;master-size t2.small \<br>&ndash;topology private \<br>&ndash;networking weave \<br>&ndash;image kope.io/k8s-1.6-debian-jessie-amd64-hvm-ebs-2017-05-02 \<br>&ndash;kubernetes-version=1.6.2 \<br>&ndash;yes</p><p>This will spin up a HA kubernetes cluster with three master nodes spread across three availability zones, and three worker nodes, also spread across three availability zones. Wait for the cluster to spin up and verify that you can get a list of nodes (the above will spin up a private cluster and you therefore need to add a public DNS entry in Route53 to access the Kubernetes api from the internet. Just duplicate the entry Kops created in the private zone in the public zone):</p><p>$ kubectl get nodes -L kubernetes.io/role<br>NAME STATUS AGE VERSION ROLE<br>ip-172-20-126-152.eu-west-1.compute.internal Ready 1m v1.6.2 master<br>ip-172-20-127-107.eu-west-1.compute.internal Ready 36s v1.6.2 node<br>ip-172-20-55-11.eu-west-1.compute.internal Ready 1m v1.6.2 master<br>ip-172-20-63-108.eu-west-1.compute.internal Ready 20s v1.6.2 node<br>ip-172-20-69-180.eu-west-1.compute.internal Ready 22s v1.6.2 node<br>ip-172-20-89-92.eu-west-1.compute.internal Ready 2m v1.6.2 master</p><p>Verify the kubernetes version with</p><p>$ kubectl version<br>Client Version: version.Info{Major:&ldquo;1&rdquo;, Minor:&ldquo;7&rdquo;, GitVersion:&ldquo;v1.7.4&rdquo;, GitCommit:&ldquo;793658f2d7ca7f064d2bdf606519f9fe1229c381&rdquo;, GitTreeState:&ldquo;clean&rdquo;, BuildDate:&ldquo;2017-08-17T08:48:23Z&rdquo;, GoVersion:&ldquo;go1.8.3&rdquo;, Compiler:&ldquo;gc&rdquo;, Platform:&ldquo;darwin/amd64&rdquo;}<br>Server Version: version.Info{Major:&ldquo;1&rdquo;, Minor:&ldquo;6&rdquo;, GitVersion:&ldquo;v1.6.2&rdquo;, GitCommit:&ldquo;477efc3cbe6a7effca06bd1452fa356e2201e1ee&rdquo;, GitTreeState:&ldquo;clean&rdquo;, BuildDate:&ldquo;2017-04-19T20:22:08Z&rdquo;, GoVersion:&ldquo;go1.7.5&rdquo;, Compiler:&ldquo;gc&rdquo;, Platform:&ldquo;linux/amd64&rdquo;}</p><h3 id=upgrading-your-cluster-configuration>Upgrading your cluster configuration</h3><p>We are now ready to update our cluster. Kops comes with a simple feature for upgrading your cluster with the <code>upgrade</code> command.</p><p>Before we run the update command we need to go fetch the latest version of Kops (in the time of writing 1.7.1, link: <a href=https://github.com/kubernetes/kops/releases/tag/1.7.1>https://github.com/kubernetes/kops/releases/tag/1.7.1</a>)</p><p>Now run the <code>upgrade</code> command as follows:</p><p>$ kops upgrade cluster $KOPS_NAME<br>ITEM PROPERTY OLD NEW<br>Cluster KubernetesVersion 1.6.2 1.7.10<br>InstanceGroup/master-eu-west-1a Image kope.io/k8s-1.6-debian-jessie-amd64-hvm-ebs-2017-05-02 kope.io/k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-07-28<br>InstanceGroup/master-eu-west-1b Image kope.io/k8s-1.6-debian-jessie-amd64-hvm-ebs-2017-05-02 kope.io/k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-07-28<br>InstanceGroup/master-eu-west-1c Image kope.io/k8s-1.6-debian-jessie-amd64-hvm-ebs-2017-05-02 kope.io/k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-07-28<br>InstanceGroup/nodes Image kope.io/k8s-1.6-debian-jessie-amd64-hvm-ebs-2017-05-02 kope.io/k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-07-28</p><p>Must specify &ndash;yes to perform upgrade</p><p>Verify the options that Kops provides, and if satisfied, add <code>--yes</code> to the command for upgrading the cluster configuration.</p><p>$ kops upgrade cluster $KOPS_NAME &ndash;yes<br>ITEM PROPERTY OLD NEW<br>Cluster KubernetesVersion 1.6.2 1.7.10<br>InstanceGroup/master-eu-west-1a Image kope.io/k8s-1.6-debian-jessie-amd64-hvm-ebs-2017-05-02 kope.io/k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-07-28<br>InstanceGroup/master-eu-west-1b Image kope.io/k8s-1.6-debian-jessie-amd64-hvm-ebs-2017-05-02 kope.io/k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-07-28<br>InstanceGroup/master-eu-west-1c Image kope.io/k8s-1.6-debian-jessie-amd64-hvm-ebs-2017-05-02 kope.io/k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-07-28<br>InstanceGroup/nodes Image kope.io/k8s-1.6-debian-jessie-amd64-hvm-ebs-2017-05-02 kope.io/k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-07-28</p><p>Updates applied to configuration.<br>You can now apply these changes, using `kops update cluster $KOPS_NAME`</p><p>Next, thing is to push this new configuration to AWS, with the <code>update</code> command.</p><p>$ kops update cluster $KOPS_NAME<br>&mldr; a lot of output &mldr;</p><p>Verify the changes that Kops will make in AWS and when satisfied, accept by appending the <code>--yes</code> flag to the above command.</p><p>$ kops update cluster $KOPS_NAME &ndash;yes<br>I1112 14:00:00.516806 4406 dns.go:91] Private DNS: skipping DNS validation<br>I1112 14:00:01.500941 4406 executor.go:91] Tasks: 0 done / 103 total; 39 can run<br>I1112 14:00:02.470532 4406 executor.go:91] Tasks: 39 done / 103 total; 20 can run<br>I1112 14:00:03.252511 4406 executor.go:91] Tasks: 59 done / 103 total; 30 can run<br>I1112 14:00:05.935267 4406 executor.go:91] Tasks: 89 done / 103 total; 8 can run<br>I1112 14:00:06.095672 4406 dnsname.go:110] AliasTarget for &ldquo;&mldr;&rdquo; is &ldquo;&mldr;..&rdquo;<br>I1112 14:00:06.481874 4406 executor.go:91] Tasks: 97 done / 103 total; 6 can run<br>I1112 14:00:07.024391 4406 executor.go:91] Tasks: 103 done / 103 total; 0 can run<br>I1112 14:00:07.024442 4406 dns.go:152] Pre-creating DNS records<br>I1112 14:00:07.581470 4406 update_cluster.go:247] Exporting kubecfg for cluster<br>Kops has set your kubectl context to $KOPS_NAME</p><p>Cluster changes have been applied to the cloud.</p><p>Changes may require instances to restart: kops rolling-update cluster</p><h3 id=rolling-update-yourcluster>Rolling update your cluster</h3><p>Kops provides a couple of different options for rolling updating your cluster. The default behavior will stop your instance one by one with a default timeout until all nodes has been restarted and updated. Kops also provides a more safe rolling-update with the feature flag +DrainAndValidateRollingUpdate. This flag will first cordon the node, which will disable scheduling on the node. When this is done, Kops will drain the node which will give all pods running on the node a gracefully shutdown and rescheduling.</p><p>export KOPS_FEATURE_FLAGS=”+DrainAndValidateRollingUpdate”</p><p>This is a great feature, however it could potentially cause downtime while updating your cluster. Kops will shutdown a node before spinning up a new one. Depending on your resource capacity this may be an issue. If that’s the case, consider scaling the cluster before a production upgrade.</p><p>Another problem with this approach is draining the code, which will shutdown all pods on the instance, potentially resulting in bottlenecks on the nodes where pods will be rescheduled because of image download time. Further there’s no priority of which pods while be restarted first, meaning kube-system pods, such as kube-dns may be the last pod to get downloaded and restarted.</p><p>Instead of using these approaches, I’ve been doing it in a more manual fashion to insure no downtime during production upgrades.</p><p>To continue our previous example, let’s start out by performing a rolling-updating on our master nodes, one by one.</p><p>$ kops rolling-update cluster $KOPS_NAME &ndash;instance-group master-eu-west-1a &ndash;yes</p><p>$ kops rolling-update cluster $KOPS_NAME &ndash;instance-group master-eu-west-1b &ndash;yes</p><p>$ kops rolling-update cluster $KOPS_NAME &ndash;instance-group master-eu-west-1c &ndash;yes</p><p>You should be able to combine this to one command. However, I like to verify that everything is running before continuing.</p><p>$ kubectl get nodes -L kubernetes.io/role<br>NAME STATUS AGE VERSION ROLE<br>ip-172-20-115-44.eu-west-1.compute.internal Ready 1m v1.7.10 master<br>ip-172-20-127-107.eu-west-1.compute.internal Ready 29m v1.6.2 node<br>ip-172-20-41-196.eu-west-1.compute.internal Ready 19m v1.7.10 master<br>ip-172-20-63-108.eu-west-1.compute.internal Ready 28m v1.6.2 node<br>ip-172-20-69-180.eu-west-1.compute.internal Ready 29m v1.6.2 node<br>ip-172-20-89-246.eu-west-1.compute.internal Ready 6m v1.7.10 master</p><p>Next, we are going to rolling update our nodes one by one.</p><p>We start out by making the node unschedulable:</p><p>$ kubectl cordon</p><p>Now, one by one, delete pods and wait for them to reschedule on another node.</p><p>$ kubectl delete pod</p><p>You can use the following command to list all pods running on the particular node</p><p>$ kubectl get pods &ndash;all-namespaces -owide | grep</p><p>When all pods are moved and restartet, drain the node:</p><p>$ kubectl drain &ndash;force &ndash;ignore-daemonsets</p><p>Last thing, go to the AWS EC2 console and terminate the node.</p><p>Repeat these steps for all worker nodes.</p><p>When all worker nodes has been rolling updated, the cluster upgrade is finished, and you should see all nodes running Kubernetes 1.7.10.</p><p>$ kubectl get nodes -L kubernetes.io/role<br>NAME STATUS AGE VERSION ROLE<br>ip-172-20-115-44.eu-west-1.compute.internal Ready 10m v1.7.10 master<br>ip-172-20-117-194.eu-west-1.compute.internal Ready 1m v1.7.10 node<br>ip-172-20-41-196.eu-west-1.compute.internal Ready 28m v1.7.10 master<br>ip-172-20-42-10.eu-west-1.compute.internal Ready 5m v1.7.10 node<br>ip-172-20-67-240.eu-west-1.compute.internal Ready 3m v1.7.10 node<br>ip-172-20-89-246.eu-west-1.compute.internal Ready 15m v1.7.10 master</p><p>When finished trying the upgrade procedure, you can easily delete all resources created by Kops:</p><p>$ kops delete cluster $KOPS_NAME &ndash;yes</p><p>That’s it. Happy upgrading your Kops clusters.</p></div></div></div></div></section><footer><div class="col-md-10 offset-md-1"><div class=row><div class="col-lg-3 col-sm-6 mb-5"><a href=https://kubecloud.github.io/website><img class=img-fluid src=https://kubecloud.github.io/website/images/kubecloud.png alt=kubecloud></a></div><div class="col-lg-3 col-sm-6 mb-5"></div><div class="col-lg-3 col-sm-6 mb-5"></div><div class="col-lg-3 col-sm-6 mb-5 text-right"><h6 class=mb-4>Quick Links</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://kubecloud.github.io/website/blog>Blog</a></li><li class=mb-3><a class=text-dark href=https://kubecloud.github.io/website/about/about>About</a></li></ul></div><div class="col-6 py-4 text-left social border-top"><ul><li><a class=social-link href=https://twitter.com/phennex><i class=ti-twitter-alt></i></a></li><li><a class=social-link href=https://twitter.com/mrjensens><i class=ti-twitter-alt></i></a></li><li><a class=social-link href=https://github.com/kubecloud><i class=ti-github></i></a></li></ul></div><div class="col-6 py-4 text-right border-top">@2020 <a href=https://kubecloud.io>kubecloid.io</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://kubecloud.github.io/website/index.json"</script><script src=https://kubecloud.github.io/website/plugins/jQuery/jquery.min.js></script><script src=https://kubecloud.github.io/website/plugins/bootstrap/bootstrap.min.js></script><script src=https://kubecloud.github.io/website/plugins/search/fuse.min.js></script><script src=https://kubecloud.github.io/website/plugins/search/mark.js></script><script src=https://kubecloud.github.io/website/plugins/search/search.js></script><script src=https://kubecloud.github.io/website/js/script.min.js></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','Your ID','auto');ga('send','pageview');</script></body></html>